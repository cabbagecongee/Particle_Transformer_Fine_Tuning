apiVersion: batch/v1
kind: Job
metadata:
  name: tn-multiclass-10pct-ijd-plain-20e
  namespace: cms-ml
spec:
  backoffLimit: 2
  template:
    spec:
      restartPolicy: Never
      tolerations:
      - key: "nautilus.io/issue"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "nautilus.io/reservation"
        operator: "Exists"
        effect: "NoSchedule"

      containers:
      - name: tn-multiclass-10pct-ijd-plain-20e
        image: pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime
        env:
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: NCCL_DEBUG
          value: "WARN"
        - name: TORCH_DISTRIBUTED_DEBUG
          value: "DETAIL"
        - name: OMP_NUM_THREADS
          value: "1"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:128"
        - name: HF_HOME
          value: "/mnt/data/.cache/hf"
        - name: TRANSFORMERS_CACHE
          value: "/mnt/data/.cache/hf/transformers"
        - name: XDG_CACHE_HOME
          value: "/mnt/data/.cache"
        command:
        - sh
        - -c
        - |
          apt update && \
          apt install -y wget && \
          cd /mnt/repo/Particle_Transformer_Fine_Tunning && \
          echo "[INFO] Installing Python packages..." && \
          pip install --upgrade pip && \
          pip install accelerate \
                      numpy pandas matplotlib tqdm scikit-learn seaborn \
                      weaver-core jetnet tensorboard aiohttp requests pyarrow==14.0.1 && \
          echo "[INFO] Checking GPU availability..." && \
          nvidia-smi && \
          mkdir -p /mnt/data/output && \
          export TORCH_DISTRIBUTED_DEBUG=DETAIL && \
          accelerate launch --num_processes=4 multiclass_model.py
        volumeMounts:
        - name: git-repo
          mountPath: /mnt/repo
        - name:  tn-pvc-many-jetclass2
          mountPath: /mnt/data
        - name: dshm
          mountPath: /dev/shm
        resources:
          limits:
            memory: "64Gi"
            cpu: "4"
            nvidia.com/gpu: "4"
          requests:
            memory: "64Gi"
            cpu: "4"
            nvidia.com/gpu: "4"

      initContainers:
      - name: init-clone-repo
        image: alpine/git
        args:
        - clone
        - --single-branch
        - https://github.com/cabbagecongee/Particle_Transformer_Fine_Tunning.git
        - /mnt/repo/Particle_Transformer_Fine_Tunning
        resources:
          limits:
            memory: "900Mi"
            cpu: "4"
          requests:
            memory: "900Mi"
            cpu: "4"
        volumeMounts:
        - name: git-repo
          mountPath: /mnt/repo

      volumes:
      - name: git-repo
        emptyDir: {}
      - name:  tn-pvc-many-jetclass2
        persistentVolumeClaim:
          claimName:  tn-pvc-many-jetclass2
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                  - gpn-fiona-mizzou-2.rnet.missouri.edu
                  - rci-nrp-gpu-05.sdsu.edu
                  - node-2-2.sdsc.optiputer.net
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - Tesla-V100-SXM2-32GB
                  - NVIDIA-A40
                  - NVIDIA-L40
                  - NVIDIA-RTX-A6000
                  - Quadro-RTX-8000

