{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabbagecongee/Particle_Transformer_Fine_Tunning/blob/main/model_pipeline_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUFRK053d67",
        "outputId": "1eb70f5c-3245-4e7c-8448-9c2f89564390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: awkward in /usr/local/lib/python3.11/dist-packages (2.8.5)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (8.7.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward) (25.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward) (3.23.0)\n",
            "Requirement already satisfied: vector in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from vector) (1.26.4)\n",
            "Requirement already satisfied: packaging>=19 in /usr/local/lib/python3.11/dist-packages (from vector) (25.0)\n",
            "Requirement already satisfied: uproot in /usr/local/lib/python3.11/dist-packages (5.1.2)\n",
            "Requirement already satisfied: awkward>=2.4.6 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.8.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (25.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.23.0)\n",
            "Requirement already satisfied: weaver-core in /usr/local/lib/python3.11/dist-packages (0.4.17)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.15.3)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (6.0.2)\n",
            "Requirement already satisfied: awkward0>=0.15.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (0.15.5)\n",
            "Requirement already satisfied: uproot<5.2.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (5.1.2)\n",
            "Requirement already satisfied: awkward>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.8.5)\n",
            "Requirement already satisfied: vector>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.3)\n",
            "Requirement already satisfied: lz4>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.4.4)\n",
            "Requirement already satisfied: xxhash>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.5.0)\n",
            "Requirement already satisfied: tables>=3.6.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.18.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (8.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (3.6.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (4.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.1.3)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.1.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (4.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=1.8.0->weaver-core) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->weaver-core) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2025.7.14)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.11/dist-packages (3.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.11/dist-packages (from fabric) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.11/dist-packages (from fabric) (3.5.1)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.11/dist-packages (from fabric) (5.2.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from fabric) (1.2.18)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->fabric) (1.17.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (4.3.0)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (43.0.3)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install awkward\n",
        "!pip install vector\n",
        "!pip install uproot\n",
        "!pip install weaver-core\n",
        "!pip install fabric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rf0q3dv01WUW"
      },
      "outputs": [],
      "source": [
        "#reference: https://github.com/jet-universe/particle_transformer/blob/main/dataloader.py\n",
        "\n",
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import vector\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "import random\n",
        "vector.register_awkward()\n",
        "\n",
        "constituent_keys = [\n",
        "    \"part_px\", \"part_py\", \"part_pz\", \"part_energy\",\n",
        "    \"part_deta\", \"part_dphi\",\n",
        "    \"part_d0val\", \"part_d0err\", \"part_dzval\", \"part_dzerr\",\n",
        "    \"part_charge\",\n",
        "    \"part_isElectron\", \"part_isMuon\", \"part_isPhoton\",\n",
        "    \"part_isChargedHadron\", \"part_isNeutralHadron\",\n",
        "    \"part_pt\", \"part_eta\", \"part_phi\"\n",
        "]\n",
        "\n",
        "hlf_keys = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_energy\", \"jet_sdmass\"]\n",
        "label_key = \"jet_label\"\n",
        "\n",
        "def read_file(\n",
        "    filepath,\n",
        "    particle_features,\n",
        "    jet_features,\n",
        "    labels,\n",
        "    max_num_particles=128,\n",
        "    allowed_labels=None, \n",
        "    tau_labels=None\n",
        "):\n",
        "  def pad(a, maxlen, value=0, dtype='float32'):\n",
        "    if isinstance(a, np.ndarray) and a.ndim>=2 and a.shape[1] == maxlen:\n",
        "      return a\n",
        "    elif isinstance(a, ak.Array):\n",
        "      if a.ndim ==1:\n",
        "        a = ak.unflatten(a, 1)\n",
        "      a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
        "      return ak.values_astype(a, dtype)\n",
        "    else:\n",
        "      x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
        "      for idx, s in enumerate(a):\n",
        "        if not len(s):\n",
        "          continue\n",
        "        trunc = np.asarray(s[:maxlen], dtype=dtype)\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "      return x\n",
        "\n",
        "  table = ak.Array(ak.from_parquet(filepath))\n",
        "  num_events = len(table)\n",
        "\n",
        "  p4 = vector.zip({\n",
        "      'px': table['part_px'],\n",
        "      'py': table['part_py'],\n",
        "      'pz': table['part_pz'],\n",
        "      'E': table['part_energy']\n",
        "  })\n",
        "\n",
        "  # Shape: (num_events, 2, max_num_particles, 4)\n",
        "  v_particles = np.stack([\n",
        "      ak.to_numpy(pad(p4.px, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.py, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.pz, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.E, max_num_particles))\n",
        "  ], axis=-1)\n",
        "\n",
        "  table[\"part_pt\"] = p4.pt\n",
        "  table[\"part_eta\"] = p4.eta\n",
        "  table[\"part_phi\"] = p4.phi\n",
        "\n",
        "  y = ak.to_numpy(table[label_key]).astype('int64')\n",
        "\n",
        "  if allowed_labels is not None:\n",
        "    mask = np.isin(y, list(allowed_labels))\n",
        "    table = table[mask]\n",
        "    y = y[mask]\n",
        "    y = np.array([1 if label in tau_labels else 0 for label in y])\n",
        "\n",
        "  x_particles = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_particles)) for n in particle_features], axis=-1)\n",
        "  # x_particles = np.transpose(x_particles, (0, 2, 1))\n",
        "\n",
        "  # Mask is 1 if pt > 0 (real particle), 0 if padding\n",
        "  # Shape: (num_events, max_num_particles)\n",
        "  mask = (v_particles[:, :, 0]**2 + v_particles[:, :, 1]**2 > 1e-9).astype('float32')\n",
        "\n",
        "\n",
        "  x_jets = np.stack([ak.to_numpy(table[n]) for n in jet_features], axis=1)\n",
        "\n",
        "  return x_particles, x_jets, v_particles, mask, y\n",
        "\n",
        "\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, parquet_file, max_num_particles=128):\n",
        "    self.x_particles, self.x_jets, self.labels = read_file(\n",
        "        filepath=parquet_file,\n",
        "        particle_features=constituent_keys,\n",
        "        jet_features=hlf_keys,\n",
        "        labels=label_key\n",
        "        )\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (\n",
        "        torch.tensor(self.x_particles[idx], dtype=torch.float),\n",
        "        torch.tensor(self.x_jets[idx], dtype=torch.float),\n",
        "        torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    )\n",
        "\n",
        "\n",
        "class IterableJetDataset(IterableDataset):\n",
        "  def __init__(self, filepaths, shuffle_files=True, max_num_particles=128, allowed_labels=None, tau_labels=None):\n",
        "    self.filepaths = filepaths\n",
        "    self.shuffle_files = shuffle_files\n",
        "    self.max_num_particles = max_num_particles\n",
        "    self.allowed_labels = allowed_labels\n",
        "    self.tau_labels = tau_labels\n",
        "\n",
        "\n",
        "  def parse_files(self, filepath):\n",
        "    x_particles, x_jets, v_particles, mask, labels = read_file(\n",
        "      filepath=filepath,\n",
        "      particle_features=constituent_keys,\n",
        "      jet_features=hlf_keys,\n",
        "      labels=label_key,\n",
        "      max_num_particles=self.max_num_particles,\n",
        "      allowed_labels=self.allowed_labels, \n",
        "      tau_labels=self.tau_labels\n",
        "      )\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "      yield(\n",
        "          torch.tensor(x_particles[i], dtype=torch.float).clone(),\n",
        "          torch.tensor(x_jets[i], dtype=torch.float).clone(),\n",
        "          torch.tensor(v_particles[i], dtype=torch.float).clone(),\n",
        "           torch.tensor(mask[i], dtype=torch.float),\n",
        "          torch.tensor(labels[i], dtype=torch.long).clone(),\n",
        "      )\n",
        "  # def __iter__(self):\n",
        "  #   if self.shuffle_files:\n",
        "  #     random.shuffle(self.filepaths)\n",
        "  #   for filepath in self.filepaths:\n",
        "  #     yield from self.parse_files(filepath)\n",
        "\n",
        "  def __iter__(self):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "\n",
        "    if self.shuffle_files: #shuffles for each epoch\n",
        "        random.shuffle(self.filepaths)\n",
        "\n",
        "    # Determine which files this worker should process\n",
        "    if worker_info is None:\n",
        "        # Case 1: Single-process loading (num_workers=0)\n",
        "        # The main process handles all files.\n",
        "        file_list = self.filepaths\n",
        "    else:\n",
        "        # Case 2: Multi-process loading\n",
        "        # Split the workload. Each worker gets a unique slice of the file list.\n",
        "        worker_id = worker_info.id\n",
        "        num_workers = worker_info.num_workers\n",
        "        file_list = self.filepaths[worker_id::num_workers]\n",
        "\n",
        "    for filepath in file_list:\n",
        "        try:\n",
        "            yield from self.parse_files(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Worker {worker_info.id if worker_info else 0} failed to process {filepath}. Reason: {e}. Skipping file.\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohkYB5_fsURp",
        "outputId": "9a4528b7-ef76-4353-a411-8093af80525d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-16 13:09:35 URL:https://raw.githubusercontent.com/jet-universe/sophon/main/notebooks/JetClassII_example.parquet [447746/447746] -> \"JetClassII_example.parquet\" [1]\n"
          ]
        }
      ],
      "source": [
        "! wget --no-verbose https://github.com/jet-universe/sophon/raw/main/notebooks/JetClassII_example.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLFre7YdrtsO",
        "outputId": "ac6bf731-7f1e-46ac-a26b-9ff2b35bf095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128, 19])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32, 128, 4])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "filepaths = [\"JetClassII_example.parquet\"] \n",
        "dataset = IterableJetDataset(filepaths, max_num_particles=128)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "for x_particles, x_jets, v_particles, mask, labels in dataloader:\n",
        "    print(x_particles.shape) #[batch size, num jets, particle features]\n",
        "    print(x_jets.shape) # [batch size, num jets, jet features (HLFs)]\n",
        "    print(v_particles.shape)\n",
        "    print(mask.shape)\n",
        "    print(labels.shape) # [batch size]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'IterableJetDataset' object has no attribute 'labels'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m unique_classes = np.unique(\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnique classes found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNum classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'IterableJetDataset' object has no attribute 'labels'"
          ]
        }
      ],
      "source": [
        "unique_classes = np.unique(dataset.labels)\n",
        "print(f\"Unique classes found: {unique_classes} \")\n",
        "print(f\"Num classes: {len(unique_classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbfNzEvk1_Hc"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "''' Particle Transformer (ParT)\n",
        "\n",
        "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
        "'''\n",
        "\n",
        "#source: https://github.com/hqucms/weaver-core/blob/main/weaver/nn/model/ParticleTransformer.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from weaver.utils.logger import _logger\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_phi(a, b):\n",
        "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_r2(eta1, phi1, eta2, phi2):\n",
        "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
        "\n",
        "\n",
        "def to_pt2(x, eps=1e-8):\n",
        "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        pt2 = pt2.clamp(min=eps)\n",
        "    return pt2\n",
        "\n",
        "\n",
        "def to_m2(x, eps=1e-8):\n",
        "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        m2 = m2.clamp(min=eps)\n",
        "    return m2\n",
        "\n",
        "\n",
        "def atan2(y, x):\n",
        "    sx = torch.sign(x)\n",
        "    sy = torch.sign(y)\n",
        "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
        "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
        "    return atan_part + pi_part\n",
        "\n",
        "\n",
        "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
        "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
        "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
        "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
        "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
        "    if not return_mass:\n",
        "        return torch.cat((pt, rapidity, phi), dim=1)\n",
        "    else:\n",
        "        m = torch.sqrt(to_m2(x, eps=eps))\n",
        "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
        "\n",
        "\n",
        "def boost(x, boostp4, eps=1e-8):\n",
        "    # boost x to the rest frame of boostp4\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
        "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
        "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
        "    gamma2 = (gamma - 1) / b2\n",
        "    gamma2.masked_fill_(b2 == 0, 0)\n",
        "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
        "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
        "    return v\n",
        "\n",
        "\n",
        "def p3_norm(p, eps=1e-8):\n",
        "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
        "\n",
        "\n",
        "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
        "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "\n",
        "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
        "    lndelta = torch.log(delta.clamp(min=eps))\n",
        "    if num_outputs == 1:\n",
        "        return lndelta\n",
        "\n",
        "    if num_outputs > 1:\n",
        "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
        "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
        "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
        "        outputs = [lnkt, lnz, lndelta]\n",
        "\n",
        "    if num_outputs > 3:\n",
        "        xij = xi + xj\n",
        "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
        "        outputs.append(lnm2)\n",
        "\n",
        "    if num_outputs > 4:\n",
        "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
        "        outputs.append(lnds2)\n",
        "\n",
        "    # the following features are not symmetric for (i, j)\n",
        "    if num_outputs > 5:\n",
        "        xj_boost = boost(xj, xij)\n",
        "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
        "        outputs.append(costheta)\n",
        "\n",
        "    if num_outputs > 6:\n",
        "        deltarap = rapi - rapj\n",
        "        deltaphi = delta_phi(phii, phij)\n",
        "        outputs += [deltarap, deltaphi]\n",
        "\n",
        "    assert (len(outputs) == num_outputs)\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "def build_sparse_tensor(uu, idx, seq_len):\n",
        "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
        "    # return: (N, C, seq_len, seq_len)\n",
        "    batch_size, num_fts, num_pairs = uu.size()\n",
        "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
        "    i = torch.cat((\n",
        "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
        "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
        "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "    ), dim=0)\n",
        "    return torch.sparse_coo_tensor(\n",
        "        i, uu.flatten(),\n",
        "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
        "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SequenceTrimmer(nn.Module):\n",
        "\n",
        "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.enabled = enabled\n",
        "        self.target = target\n",
        "        self._counter = 0\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # uu: (N, C', P, P)\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(x[:, :1])\n",
        "        mask = mask.bool()\n",
        "\n",
        "        if self.enabled:\n",
        "            if self._counter < 5:\n",
        "                self._counter += 1\n",
        "            else:\n",
        "                if self.training:\n",
        "                    q = min(1, random.uniform(*self.target))\n",
        "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
        "                    rand = torch.rand_like(mask.type_as(x))\n",
        "                    rand.masked_fill_(~mask, -1)\n",
        "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
        "                    mask = torch.gather(mask, -1, perm)\n",
        "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
        "                    if v is not None:\n",
        "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
        "                    if uu is not None:\n",
        "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
        "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
        "                else:\n",
        "                    maxlen = mask.sum(dim=-1).max()\n",
        "                maxlen = max(maxlen, 1)\n",
        "                if maxlen < mask.size(-1):\n",
        "                    mask = mask[:, :, :maxlen]\n",
        "                    x = x[:, :, :maxlen]\n",
        "                    if v is not None:\n",
        "                        v = v[:, :, :maxlen]\n",
        "                    if uu is not None:\n",
        "                        uu = uu[:, :, :maxlen, :maxlen]\n",
        "\n",
        "        return x, v, mask, uu\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
        "        module_list = []\n",
        "        for dim in dims:\n",
        "            module_list.extend([\n",
        "                nn.LayerNorm(input_dim),\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "            ])\n",
        "            input_dim = dim\n",
        "        self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_bn is not None:\n",
        "            # x: (batch, embed_dim, seq_len)\n",
        "            x = self.input_bn(x)\n",
        "            x = x.permute(2, 0, 1).contiguous()\n",
        "        # x: (seq_len, batch, embed_dim)\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PairEmbed(nn.Module):\n",
        "    def __init__(\n",
        "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
        "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
        "            normalize_input=True, activation='gelu', eps=1e-8,\n",
        "            for_onnx=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pairwise_lv_dim = pairwise_lv_dim\n",
        "        self.pairwise_input_dim = pairwise_input_dim\n",
        "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
        "        self.remove_self_pair = remove_self_pair\n",
        "        self.mode = mode\n",
        "        self.for_onnx = for_onnx\n",
        "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
        "        self.out_dim = dims[-1]\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
        "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "            for dim in dims:\n",
        "                module_list.extend([\n",
        "                    nn.Conv1d(input_dim, dim, 1),\n",
        "                    nn.BatchNorm1d(dim),\n",
        "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                ])\n",
        "                input_dim = dim\n",
        "            if use_pre_activation_pair:\n",
        "                module_list = module_list[:-1]\n",
        "            self.embed = nn.Sequential(*module_list)\n",
        "        elif self.mode == 'sum':\n",
        "            if pairwise_lv_dim > 0:\n",
        "                input_dim = pairwise_lv_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "            if pairwise_input_dim > 0:\n",
        "                input_dim = pairwise_input_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.fts_embed = nn.Sequential(*module_list)\n",
        "        else:\n",
        "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
        "\n",
        "    def forward(self, x, uu=None):\n",
        "        # x: (batch, v_dim, seq_len)\n",
        "        # uu: (batch, v_dim, seq_len, seq_len)\n",
        "        assert (x is not None or uu is not None)\n",
        "        with torch.no_grad():\n",
        "            if x is not None:\n",
        "                batch_size, _, seq_len = x.size()\n",
        "            else:\n",
        "                batch_size, _, seq_len, _ = uu.size()\n",
        "            if self.is_symmetric and not self.for_onnx:\n",
        "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
        "                                          device=(x if x is not None else uu).device)\n",
        "                if x is not None:\n",
        "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
        "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    xj = x[:, :, j, i]\n",
        "                    x = self.pairwise_lv_fts(xi, xj)\n",
        "                if uu is not None:\n",
        "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    uu = uu[:, :, i, j]\n",
        "            else:\n",
        "                if x is not None:\n",
        "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
        "                    if self.remove_self_pair:\n",
        "                        i = torch.arange(0, seq_len, device=x.device)\n",
        "                        x[:, :, i, i] = 0\n",
        "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
        "                if uu is not None:\n",
        "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
        "            if self.mode == 'concat':\n",
        "                if x is None:\n",
        "                    pair_fts = uu\n",
        "                elif uu is None:\n",
        "                    pair_fts = x\n",
        "                else:\n",
        "                    pair_fts = torch.cat((x, uu), dim=1)\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
        "        elif self.mode == 'sum':\n",
        "            if x is None:\n",
        "                elements = self.fts_embed(uu)\n",
        "            elif uu is None:\n",
        "                elements = self.embed(x)\n",
        "            else:\n",
        "                elements = self.embed(x) + self.fts_embed(uu)\n",
        "\n",
        "        if self.is_symmetric and not self.for_onnx:\n",
        "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
        "            y[:, :, i, j] = elements\n",
        "            y[:, :, j, i] = elements\n",
        "        else:\n",
        "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
        "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                 add_bias_kv=False, activation='gelu',\n",
        "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ffn_dim = embed_dim * ffn_ratio\n",
        "\n",
        "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=attn_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "        )\n",
        "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
        "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
        "        self.act_dropout = nn.Dropout(activation_dropout)\n",
        "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
        "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
        "\n",
        "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
        "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
        "\n",
        "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
        "            padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape `(batch, seq_len)` where padding\n",
        "                elements are indicated by ``1``.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "\n",
        "        if x_cls is not None:\n",
        "            with torch.no_grad():\n",
        "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
        "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
        "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
        "            residual = x_cls\n",
        "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
        "            u = self.pre_attn_norm(u)\n",
        "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
        "        else:\n",
        "            residual = x\n",
        "            x = self.pre_attn_norm(x)\n",
        "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask)[0]  # (seq_len, batch, embed_dim)\n",
        "\n",
        "        if self.c_attn is not None:\n",
        "            tgt_len = x.size(0)\n",
        "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
        "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
        "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
        "        if self.post_attn_norm is not None:\n",
        "            x = self.post_attn_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.pre_fc_norm(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act_dropout(x)\n",
        "        if self.post_fc_norm is not None:\n",
        "            x = self.post_fc_norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        if self.w_resid is not None:\n",
        "            residual = torch.mul(self.w_resid, residual)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ParticleTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            if self.for_inference:\n",
        "                output = torch.softmax(output, dim=1)\n",
        "            # print('output:\\n', output)\n",
        "            return output\n",
        "\n",
        "\n",
        "class ParticleTransformerTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask)\n",
        "\n",
        "\n",
        "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if pf_uu_idx is not None:\n",
        "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
        "\n",
        "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
        "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask, uu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BAL5AfiWSKIM"
      },
      "outputs": [],
      "source": [
        "class ParticleTransformerBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AmfBoIrQzR7",
        "outputId": "cde90122-dbb6-4e23-bbff-d0d197f8b083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ParticleTransformerBackbone(\n",
            "  (trimmer): SequenceTrimmer()\n",
            "  (embed): Embed(\n",
            "    (input_bn): BatchNorm1d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (embed): Sequential(\n",
            "      (0): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=19, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (7): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (8): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (pair_embed): PairEmbed(\n",
            "    (embed): Sequential(\n",
            "      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
            "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): GELU(approximate='none')\n",
            "      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): GELU(approximate='none')\n",
            "      (10): Conv1d(64, 2, kernel_size=(1,), stride=(1,))\n",
            "      (11): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (cls_blocks): ModuleList(\n",
            "    (0-1): 2 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# test model\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=20,        # number of jet classes in JetClassII\n",
        "    num_heads=2,           # match default\n",
        "    num_layers=4,          # match default\n",
        ")\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_K5_0EqlSt1r",
        "outputId": "cf4a65c6-9a35-4831-d3da-486441735524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100: 0it [00:00, ?it/s]/var/folders/x5/nk2s0y9j023dsj349jm6v7fh0000gn/T/ipykernel_52199/2604744380.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
            "Epoch 1/100: 4it [00:06,  1.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 5.2555, Accuracy: 0.0300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100: 4it [00:03,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Loss: 4.2832, Accuracy: 0.1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100: 4it [00:03,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100, Loss: 4.0118, Accuracy: 0.1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100: 4it [00:02,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100, Loss: 3.8418, Accuracy: 0.1200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100: 4it [00:03,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100, Loss: 3.6705, Accuracy: 0.1400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100: 4it [00:02,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100, Loss: 3.5635, Accuracy: 0.1400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/100: 4it [00:02,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100, Loss: 3.4178, Accuracy: 0.1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/100: 4it [00:02,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100, Loss: 3.2672, Accuracy: 0.1300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/100: 4it [00:02,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100, Loss: 3.1742, Accuracy: 0.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/100: 4it [00:02,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 3.0887, Accuracy: 0.1300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/100: 4it [00:02,  1.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100, Loss: 3.1085, Accuracy: 0.0900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/100: 4it [00:02,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100, Loss: 3.0283, Accuracy: 0.1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/100: 4it [00:02,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/100, Loss: 3.1054, Accuracy: 0.1300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/100: 4it [00:02,  1.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/100, Loss: 2.8757, Accuracy: 0.1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/100: 4it [00:02,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100, Loss: 2.8509, Accuracy: 0.1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/100: 4it [00:02,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100, Loss: 2.7609, Accuracy: 0.1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/100: 4it [00:02,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100, Loss: 2.6311, Accuracy: 0.1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/100: 4it [00:02,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100, Loss: 2.4617, Accuracy: 0.1800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/100: 4it [00:02,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100, Loss: 2.3544, Accuracy: 0.2200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/100: 4it [00:02,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/100, Loss: 2.2485, Accuracy: 0.2900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/100: 4it [00:02,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/100, Loss: 2.1605, Accuracy: 0.2900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/100: 4it [00:02,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/100, Loss: 2.0541, Accuracy: 0.2800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/100: 4it [00:02,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/100, Loss: 1.9710, Accuracy: 0.3400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/100: 4it [00:02,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100, Loss: 1.9237, Accuracy: 0.3400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/100: 4it [00:02,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/100, Loss: 1.7168, Accuracy: 0.3900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/100: 4it [00:02,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/100, Loss: 1.5774, Accuracy: 0.4500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/100: 4it [00:02,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/100, Loss: 1.4650, Accuracy: 0.4800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/100: 4it [00:03,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/100, Loss: 1.4087, Accuracy: 0.5200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/100: 4it [00:02,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/100, Loss: 1.3801, Accuracy: 0.4700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/100: 4it [00:02,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100, Loss: 1.2786, Accuracy: 0.5700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/100: 4it [00:03,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/100, Loss: 1.3189, Accuracy: 0.5100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/100: 4it [00:02,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/100, Loss: 1.4602, Accuracy: 0.4400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/100: 4it [00:02,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/100, Loss: 1.4792, Accuracy: 0.4100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/100: 4it [00:02,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/100, Loss: 1.2002, Accuracy: 0.6200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/100: 4it [00:03,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100, Loss: 1.2335, Accuracy: 0.5900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/100: 4it [00:02,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/100, Loss: 1.1735, Accuracy: 0.5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/100: 4it [00:03,  1.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100, Loss: 1.0368, Accuracy: 0.6600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/100: 4it [00:03,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/100, Loss: 0.8800, Accuracy: 0.7200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/100: 4it [00:03,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100, Loss: 0.8798, Accuracy: 0.7300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/100: 4it [00:03,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/100, Loss: 0.8100, Accuracy: 0.7700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/100: 4it [00:02,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/100, Loss: 0.7986, Accuracy: 0.7400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/100: 4it [00:02,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/100, Loss: 0.7258, Accuracy: 0.7500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/100: 4it [00:03,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/100, Loss: 0.6057, Accuracy: 0.8100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/100: 4it [00:02,  1.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/100, Loss: 0.5204, Accuracy: 0.9000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/100: 4it [00:03,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/100, Loss: 0.4663, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/100: 4it [00:02,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/100, Loss: 0.4674, Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/100: 4it [00:03,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/100, Loss: 0.4755, Accuracy: 0.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/100: 4it [00:02,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/100, Loss: 0.3936, Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/100: 4it [00:02,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/100, Loss: 0.2859, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/100: 4it [00:03,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/100, Loss: 0.2467, Accuracy: 0.9900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 51/100: 4it [00:03,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51/100, Loss: 0.2103, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 52/100: 4it [00:02,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52/100, Loss: 0.1766, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 53/100: 4it [00:03,  1.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53/100, Loss: 0.1708, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 54/100: 4it [00:03,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54/100, Loss: 0.1454, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 55/100: 4it [00:02,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/100, Loss: 0.1360, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 56/100: 4it [00:02,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56/100, Loss: 0.1368, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 57/100: 4it [00:02,  1.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57/100, Loss: 0.1110, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 58/100: 4it [00:01,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/100, Loss: 0.0992, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 59/100: 4it [00:02,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59/100, Loss: 0.1146, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 60/100: 4it [00:03,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/100, Loss: 0.0972, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 61/100: 4it [00:02,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61/100, Loss: 0.0902, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 62/100: 4it [00:02,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62/100, Loss: 0.1380, Accuracy: 0.9900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 63/100: 4it [00:01,  2.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63/100, Loss: 0.3823, Accuracy: 0.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 64/100: 4it [00:02,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64/100, Loss: 0.6253, Accuracy: 0.7900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 65/100: 4it [00:02,  1.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65/100, Loss: 0.5671, Accuracy: 0.8200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 66/100: 4it [00:02,  1.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66/100, Loss: 0.5479, Accuracy: 0.8400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 67/100: 4it [00:02,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67/100, Loss: 0.4596, Accuracy: 0.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 68/100: 4it [00:02,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68/100, Loss: 0.4696, Accuracy: 0.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 69/100: 4it [00:02,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69/100, Loss: 0.3622, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 70/100: 4it [00:02,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70/100, Loss: 0.3694, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 71/100: 4it [00:01,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71/100, Loss: 0.3104, Accuracy: 0.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 72/100: 4it [00:02,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72/100, Loss: 0.3720, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 73/100: 4it [00:01,  2.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73/100, Loss: 0.3407, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 74/100: 4it [00:01,  2.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74/100, Loss: 0.2941, Accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 75/100: 4it [00:01,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75/100, Loss: 0.1815, Accuracy: 0.9500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 76/100: 4it [00:02,  1.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76/100, Loss: 0.1089, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 77/100: 4it [00:02,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77/100, Loss: 0.0915, Accuracy: 0.9800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 78/100: 4it [00:02,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78/100, Loss: 0.0575, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 79/100: 4it [00:02,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/100, Loss: 0.0536, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 80/100: 4it [00:02,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80/100, Loss: 0.0393, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 81/100: 4it [00:02,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81/100, Loss: 0.0317, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 82/100: 4it [00:02,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82/100, Loss: 0.0279, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 83/100: 4it [00:02,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83/100, Loss: 0.0236, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 84/100: 4it [00:02,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84/100, Loss: 0.0211, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 85/100: 4it [00:02,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85/100, Loss: 0.0198, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 86/100: 4it [00:02,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86/100, Loss: 0.0203, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 87/100: 4it [00:02,  1.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87/100, Loss: 0.0171, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 88/100: 4it [00:02,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88/100, Loss: 0.0163, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 89/100: 4it [00:02,  1.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89/100, Loss: 0.0156, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 90/100: 4it [00:02,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90/100, Loss: 0.0148, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 91/100: 4it [00:02,  1.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91/100, Loss: 0.0146, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 92/100: 4it [00:01,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92/100, Loss: 0.0136, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 93/100: 4it [00:03,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93/100, Loss: 0.0132, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 94/100: 4it [00:03,  1.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94/100, Loss: 0.0129, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 95/100: 4it [00:02,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95/100, Loss: 0.0124, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 96/100: 4it [00:02,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96/100, Loss: 0.0122, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 97/100: 4it [00:02,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97/100, Loss: 0.0117, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 98/100: 4it [00:01,  2.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98/100, Loss: 0.0115, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 99/100: 4it [00:02,  1.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99/100, Loss: 0.0112, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 100/100: 4it [00:02,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/100, Loss: 0.0108, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train.py\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3\n",
        "EPOCHS = 100\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "filepaths = [\"JetClassII_example.parquet\"]\n",
        "train_dataset = IterableJetDataset(filepaths, max_num_particles=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=188       # number of jet classes in JetClassII\n",
        "  ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "acc = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  num_batches = 0 \n",
        "  for x_particles, x_jets, v_particles, mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "    x_particles = x_particles.to(device)\n",
        "    v_particles = v_particles.to(device)\n",
        "    mask = mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(\n",
        "            x=x_particles.transpose(1, 2),\n",
        "            v=v_particles.transpose(1, 2),\n",
        "            mask=mask.unsqueeze(1)\n",
        "        )\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    _, pred = outputs.max(1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "    num_batches += 1\n",
        "\n",
        "  epoch_acc = correct / total if total > 0 else 0\n",
        "  epoch_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "  acc.append(epoch_acc)\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), acc, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqUTdB_IsOuT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN57tMsUk+m6rs26eSdvPzt",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
