{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabbagecongee/Particle_Transformer_Fine_Tunning/blob/main/model_pipeline_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUFRK053d67",
        "outputId": "1eb70f5c-3245-4e7c-8448-9c2f89564390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: awkward in /usr/local/lib/python3.11/dist-packages (2.8.5)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (8.7.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward) (25.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward) (3.23.0)\n",
            "Requirement already satisfied: vector in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from vector) (1.26.4)\n",
            "Requirement already satisfied: packaging>=19 in /usr/local/lib/python3.11/dist-packages (from vector) (25.0)\n",
            "Requirement already satisfied: uproot in /usr/local/lib/python3.11/dist-packages (5.1.2)\n",
            "Requirement already satisfied: awkward>=2.4.6 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.8.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (25.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.23.0)\n",
            "Requirement already satisfied: weaver-core in /usr/local/lib/python3.11/dist-packages (0.4.17)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.15.3)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (6.0.2)\n",
            "Requirement already satisfied: awkward0>=0.15.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (0.15.5)\n",
            "Requirement already satisfied: uproot<5.2.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (5.1.2)\n",
            "Requirement already satisfied: awkward>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.8.5)\n",
            "Requirement already satisfied: vector>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.3)\n",
            "Requirement already satisfied: lz4>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.4.4)\n",
            "Requirement already satisfied: xxhash>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.5.0)\n",
            "Requirement already satisfied: tables>=3.6.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.18.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (8.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (3.6.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (4.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.1.3)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.1.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (4.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=1.8.0->weaver-core) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->weaver-core) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2025.7.14)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.11/dist-packages (3.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.11/dist-packages (from fabric) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.11/dist-packages (from fabric) (3.5.1)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.11/dist-packages (from fabric) (5.2.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from fabric) (1.2.18)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->fabric) (1.17.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (4.3.0)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (43.0.3)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install awkward\n",
        "!pip install vector\n",
        "!pip install uproot\n",
        "!pip install weaver-core\n",
        "!pip install fabric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rf0q3dv01WUW"
      },
      "outputs": [],
      "source": [
        "#reference: https://github.com/jet-universe/particle_transformer/blob/main/dataloader.py\n",
        "\n",
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import vector\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "import random\n",
        "vector.register_awkward()\n",
        "\n",
        "constituent_keys = [\n",
        "    \"part_px\", \"part_py\", \"part_pz\", \"part_energy\",\n",
        "    \"part_deta\", \"part_dphi\",\n",
        "    \"part_d0val\", \"part_d0err\", \"part_dzval\", \"part_dzerr\",\n",
        "    \"part_charge\",\n",
        "    \"part_isElectron\", \"part_isMuon\", \"part_isPhoton\",\n",
        "    \"part_isChargedHadron\", \"part_isNeutralHadron\",\n",
        "    \"part_pt\", \"part_eta\", \"part_phi\"\n",
        "]\n",
        "\n",
        "hlf_keys = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_energy\", \"jet_sdmass\"]\n",
        "label_key = \"jet_label\"\n",
        "\n",
        "def read_file(\n",
        "    filepath,\n",
        "    particle_features,\n",
        "    jet_features,\n",
        "    labels,\n",
        "    max_num_particles=128,\n",
        "    allowed_labels=None, \n",
        "    tau_labels=None\n",
        "):\n",
        "  def pad(a, maxlen, value=0, dtype='float32'):\n",
        "    if isinstance(a, np.ndarray) and a.ndim>=2 and a.shape[1] == maxlen:\n",
        "      return a\n",
        "    elif isinstance(a, ak.Array):\n",
        "      if a.ndim ==1:\n",
        "        a = ak.unflatten(a, 1)\n",
        "      a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
        "      return ak.values_astype(a, dtype)\n",
        "    else:\n",
        "      x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
        "      for idx, s in enumerate(a):\n",
        "        if not len(s):\n",
        "          continue\n",
        "        trunc = np.asarray(s[:maxlen], dtype=dtype)\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "      return x\n",
        "\n",
        "  table = ak.Array(ak.from_parquet(filepath))\n",
        "  num_events = len(table)\n",
        "\n",
        "  p4 = vector.zip({\n",
        "      'px': table['part_px'],\n",
        "      'py': table['part_py'],\n",
        "      'pz': table['part_pz'],\n",
        "      'E': table['part_energy']\n",
        "  })\n",
        "\n",
        "  # Shape: (num_events, 2, max_num_particles, 4)\n",
        "  v_particles = np.stack([\n",
        "      ak.to_numpy(pad(p4.px, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.py, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.pz, max_num_particles)),\n",
        "      ak.to_numpy(pad(p4.E, max_num_particles))\n",
        "  ], axis=-1)\n",
        "\n",
        "  table[\"part_pt\"] = p4.pt\n",
        "  table[\"part_eta\"] = p4.eta\n",
        "  table[\"part_phi\"] = p4.phi\n",
        "\n",
        "  y = ak.to_numpy(table[label_key]).astype('int64')\n",
        "\n",
        "  if allowed_labels is not None:\n",
        "    mask = np.isin(y, list(allowed_labels))\n",
        "    table = table[mask]\n",
        "    y = y[mask]\n",
        "    y = np.array([1 if label in tau_labels else 0 for label in y])\n",
        "\n",
        "  x_particles = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_particles)) for n in particle_features], axis=-1)\n",
        "  # x_particles = np.transpose(x_particles, (0, 2, 1))\n",
        "\n",
        "  # Mask is 1 if pt > 0 (real particle), 0 if padding\n",
        "  # Shape: (num_events, max_num_particles)\n",
        "  mask = (v_particles[:, :, 0]**2 + v_particles[:, :, 1]**2 > 1e-9).astype('float32')\n",
        "\n",
        "\n",
        "  x_jets = np.stack([ak.to_numpy(table[n]) for n in jet_features], axis=1)\n",
        "\n",
        "  return x_particles, x_jets, v_particles, mask, y\n",
        "\n",
        "\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, parquet_file, max_num_particles=128):\n",
        "    self.x_particles, self.x_jets, self.labels = read_file(\n",
        "        filepath=parquet_file,\n",
        "        particle_features=constituent_keys,\n",
        "        jet_features=hlf_keys,\n",
        "        labels=label_key\n",
        "        )\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (\n",
        "        torch.tensor(self.x_particles[idx], dtype=torch.float),\n",
        "        torch.tensor(self.x_jets[idx], dtype=torch.float),\n",
        "        torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    )\n",
        "\n",
        "\n",
        "class IterableJetDataset(IterableDataset):\n",
        "  def __init__(self, filepaths, shuffle_files=True, max_num_particles=128, allowed_labels=None, tau_labels=None):\n",
        "    self.filepaths = filepaths\n",
        "    self.shuffle_files = shuffle_files\n",
        "    self.max_num_particles = max_num_particles\n",
        "    self.allowed_labels = allowed_labels\n",
        "    self.tau_labels = tau_labels\n",
        "\n",
        "\n",
        "  def parse_files(self, filepath):\n",
        "    x_particles, x_jets, v_particles, mask, labels = read_file(\n",
        "      filepath=filepath,\n",
        "      particle_features=constituent_keys,\n",
        "      jet_features=hlf_keys,\n",
        "      labels=label_key,\n",
        "      max_num_particles=self.max_num_particles,\n",
        "      allowed_labels=self.allowed_labels, \n",
        "      tau_labels=self.tau_labels\n",
        "      )\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "      yield(\n",
        "          torch.tensor(x_particles[i], dtype=torch.float).clone(),\n",
        "          torch.tensor(x_jets[i], dtype=torch.float).clone(),\n",
        "          torch.tensor(v_particles[i], dtype=torch.float).clone(),\n",
        "           torch.tensor(mask[i], dtype=torch.float),\n",
        "          torch.tensor(labels[i], dtype=torch.long).clone(),\n",
        "      )\n",
        "  # def __iter__(self):\n",
        "  #   if self.shuffle_files:\n",
        "  #     random.shuffle(self.filepaths)\n",
        "  #   for filepath in self.filepaths:\n",
        "  #     yield from self.parse_files(filepath)\n",
        "\n",
        "  def __iter__(self):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "\n",
        "    if self.shuffle_files: #shuffles for each epoch\n",
        "        random.shuffle(self.filepaths)\n",
        "\n",
        "    # Determine which files this worker should process\n",
        "    if worker_info is None:\n",
        "        # Case 1: Single-process loading (num_workers=0)\n",
        "        # The main process handles all files.\n",
        "        file_list = self.filepaths\n",
        "    else:\n",
        "        # Case 2: Multi-process loading\n",
        "        # Split the workload. Each worker gets a unique slice of the file list.\n",
        "        worker_id = worker_info.id\n",
        "        num_workers = worker_info.num_workers\n",
        "        file_list = self.filepaths[worker_id::num_workers]\n",
        "\n",
        "    for filepath in file_list:\n",
        "        try:\n",
        "            yield from self.parse_files(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Worker {worker_info.id if worker_info else 0} failed to process {filepath}. Reason: {e}. Skipping file.\")\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohkYB5_fsURp",
        "outputId": "9a4528b7-ef76-4353-a411-8093af80525d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-16 13:09:35 URL:https://raw.githubusercontent.com/jet-universe/sophon/main/notebooks/JetClassII_example.parquet [447746/447746] -> \"JetClassII_example.parquet\" [1]\n"
          ]
        }
      ],
      "source": [
        "! wget --no-verbose https://github.com/jet-universe/sophon/raw/main/notebooks/JetClassII_example.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLFre7YdrtsO",
        "outputId": "ac6bf731-7f1e-46ac-a26b-9ff2b35bf095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128, 19])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32, 128, 4])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "filepaths = [\"JetClassII_example.parquet\"] \n",
        "dataset = IterableJetDataset(filepaths, max_num_particles=128)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "for x_particles, x_jets, v_particles, mask, labels in dataloader:\n",
        "    print(x_particles.shape) #[batch size, num jets, particle features]\n",
        "    print(x_jets.shape) # [batch size, num jets, jet features (HLFs)]\n",
        "    print(v_particles.shape)\n",
        "    print(mask.shape)\n",
        "    print(labels.shape) # [batch size]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'IterableJetDataset' object has no attribute 'labels'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m unique_classes = np.unique(\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnique classes found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNum classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'IterableJetDataset' object has no attribute 'labels'"
          ]
        }
      ],
      "source": [
        "unique_classes = np.unique(dataset.labels)\n",
        "print(f\"Unique classes found: {unique_classes} \")\n",
        "print(f\"Num classes: {len(unique_classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbfNzEvk1_Hc"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "''' Particle Transformer (ParT)\n",
        "\n",
        "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
        "'''\n",
        "\n",
        "#source: https://github.com/hqucms/weaver-core/blob/main/weaver/nn/model/ParticleTransformer.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from weaver.utils.logger import _logger\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_phi(a, b):\n",
        "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_r2(eta1, phi1, eta2, phi2):\n",
        "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
        "\n",
        "\n",
        "def to_pt2(x, eps=1e-8):\n",
        "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        pt2 = pt2.clamp(min=eps)\n",
        "    return pt2\n",
        "\n",
        "\n",
        "def to_m2(x, eps=1e-8):\n",
        "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        m2 = m2.clamp(min=eps)\n",
        "    return m2\n",
        "\n",
        "\n",
        "def atan2(y, x):\n",
        "    sx = torch.sign(x)\n",
        "    sy = torch.sign(y)\n",
        "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
        "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
        "    return atan_part + pi_part\n",
        "\n",
        "\n",
        "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
        "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
        "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
        "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
        "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
        "    if not return_mass:\n",
        "        return torch.cat((pt, rapidity, phi), dim=1)\n",
        "    else:\n",
        "        m = torch.sqrt(to_m2(x, eps=eps))\n",
        "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
        "\n",
        "\n",
        "def boost(x, boostp4, eps=1e-8):\n",
        "    # boost x to the rest frame of boostp4\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
        "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
        "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
        "    gamma2 = (gamma - 1) / b2\n",
        "    gamma2.masked_fill_(b2 == 0, 0)\n",
        "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
        "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
        "    return v\n",
        "\n",
        "\n",
        "def p3_norm(p, eps=1e-8):\n",
        "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
        "\n",
        "\n",
        "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
        "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "\n",
        "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
        "    lndelta = torch.log(delta.clamp(min=eps))\n",
        "    if num_outputs == 1:\n",
        "        return lndelta\n",
        "\n",
        "    if num_outputs > 1:\n",
        "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
        "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
        "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
        "        outputs = [lnkt, lnz, lndelta]\n",
        "\n",
        "    if num_outputs > 3:\n",
        "        xij = xi + xj\n",
        "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
        "        outputs.append(lnm2)\n",
        "\n",
        "    if num_outputs > 4:\n",
        "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
        "        outputs.append(lnds2)\n",
        "\n",
        "    # the following features are not symmetric for (i, j)\n",
        "    if num_outputs > 5:\n",
        "        xj_boost = boost(xj, xij)\n",
        "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
        "        outputs.append(costheta)\n",
        "\n",
        "    if num_outputs > 6:\n",
        "        deltarap = rapi - rapj\n",
        "        deltaphi = delta_phi(phii, phij)\n",
        "        outputs += [deltarap, deltaphi]\n",
        "\n",
        "    assert (len(outputs) == num_outputs)\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "def build_sparse_tensor(uu, idx, seq_len):\n",
        "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
        "    # return: (N, C, seq_len, seq_len)\n",
        "    batch_size, num_fts, num_pairs = uu.size()\n",
        "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
        "    i = torch.cat((\n",
        "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
        "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
        "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "    ), dim=0)\n",
        "    return torch.sparse_coo_tensor(\n",
        "        i, uu.flatten(),\n",
        "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
        "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SequenceTrimmer(nn.Module):\n",
        "\n",
        "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.enabled = enabled\n",
        "        self.target = target\n",
        "        self._counter = 0\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # uu: (N, C', P, P)\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(x[:, :1])\n",
        "        mask = mask.bool()\n",
        "\n",
        "        if self.enabled:\n",
        "            if self._counter < 5:\n",
        "                self._counter += 1\n",
        "            else:\n",
        "                if self.training:\n",
        "                    q = min(1, random.uniform(*self.target))\n",
        "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
        "                    rand = torch.rand_like(mask.type_as(x))\n",
        "                    rand.masked_fill_(~mask, -1)\n",
        "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
        "                    mask = torch.gather(mask, -1, perm)\n",
        "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
        "                    if v is not None:\n",
        "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
        "                    if uu is not None:\n",
        "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
        "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
        "                else:\n",
        "                    maxlen = mask.sum(dim=-1).max()\n",
        "                maxlen = max(maxlen, 1)\n",
        "                if maxlen < mask.size(-1):\n",
        "                    mask = mask[:, :, :maxlen]\n",
        "                    x = x[:, :, :maxlen]\n",
        "                    if v is not None:\n",
        "                        v = v[:, :, :maxlen]\n",
        "                    if uu is not None:\n",
        "                        uu = uu[:, :, :maxlen, :maxlen]\n",
        "\n",
        "        return x, v, mask, uu\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
        "        module_list = []\n",
        "        for dim in dims:\n",
        "            module_list.extend([\n",
        "                nn.LayerNorm(input_dim),\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "            ])\n",
        "            input_dim = dim\n",
        "        self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_bn is not None:\n",
        "            # x: (batch, embed_dim, seq_len)\n",
        "            x = self.input_bn(x)\n",
        "            x = x.permute(2, 0, 1).contiguous()\n",
        "        # x: (seq_len, batch, embed_dim)\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PairEmbed(nn.Module):\n",
        "    def __init__(\n",
        "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
        "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
        "            normalize_input=True, activation='gelu', eps=1e-8,\n",
        "            for_onnx=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pairwise_lv_dim = pairwise_lv_dim\n",
        "        self.pairwise_input_dim = pairwise_input_dim\n",
        "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
        "        self.remove_self_pair = remove_self_pair\n",
        "        self.mode = mode\n",
        "        self.for_onnx = for_onnx\n",
        "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
        "        self.out_dim = dims[-1]\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
        "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "            for dim in dims:\n",
        "                module_list.extend([\n",
        "                    nn.Conv1d(input_dim, dim, 1),\n",
        "                    nn.BatchNorm1d(dim),\n",
        "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                ])\n",
        "                input_dim = dim\n",
        "            if use_pre_activation_pair:\n",
        "                module_list = module_list[:-1]\n",
        "            self.embed = nn.Sequential(*module_list)\n",
        "        elif self.mode == 'sum':\n",
        "            if pairwise_lv_dim > 0:\n",
        "                input_dim = pairwise_lv_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "            if pairwise_input_dim > 0:\n",
        "                input_dim = pairwise_input_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.fts_embed = nn.Sequential(*module_list)\n",
        "        else:\n",
        "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
        "\n",
        "    def forward(self, x, uu=None):\n",
        "        # x: (batch, v_dim, seq_len)\n",
        "        # uu: (batch, v_dim, seq_len, seq_len)\n",
        "        assert (x is not None or uu is not None)\n",
        "        with torch.no_grad():\n",
        "            if x is not None:\n",
        "                batch_size, _, seq_len = x.size()\n",
        "            else:\n",
        "                batch_size, _, seq_len, _ = uu.size()\n",
        "            if self.is_symmetric and not self.for_onnx:\n",
        "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
        "                                          device=(x if x is not None else uu).device)\n",
        "                if x is not None:\n",
        "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
        "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    xj = x[:, :, j, i]\n",
        "                    x = self.pairwise_lv_fts(xi, xj)\n",
        "                if uu is not None:\n",
        "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    uu = uu[:, :, i, j]\n",
        "            else:\n",
        "                if x is not None:\n",
        "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
        "                    if self.remove_self_pair:\n",
        "                        i = torch.arange(0, seq_len, device=x.device)\n",
        "                        x[:, :, i, i] = 0\n",
        "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
        "                if uu is not None:\n",
        "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
        "            if self.mode == 'concat':\n",
        "                if x is None:\n",
        "                    pair_fts = uu\n",
        "                elif uu is None:\n",
        "                    pair_fts = x\n",
        "                else:\n",
        "                    pair_fts = torch.cat((x, uu), dim=1)\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
        "        elif self.mode == 'sum':\n",
        "            if x is None:\n",
        "                elements = self.fts_embed(uu)\n",
        "            elif uu is None:\n",
        "                elements = self.embed(x)\n",
        "            else:\n",
        "                elements = self.embed(x) + self.fts_embed(uu)\n",
        "\n",
        "        if self.is_symmetric and not self.for_onnx:\n",
        "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
        "            y[:, :, i, j] = elements\n",
        "            y[:, :, j, i] = elements\n",
        "        else:\n",
        "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
        "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                 add_bias_kv=False, activation='gelu',\n",
        "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ffn_dim = embed_dim * ffn_ratio\n",
        "\n",
        "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=attn_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "        )\n",
        "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
        "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
        "        self.act_dropout = nn.Dropout(activation_dropout)\n",
        "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
        "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
        "\n",
        "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
        "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
        "\n",
        "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
        "            padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape `(batch, seq_len)` where padding\n",
        "                elements are indicated by ``1``.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "\n",
        "        if x_cls is not None:\n",
        "            with torch.no_grad():\n",
        "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
        "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
        "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
        "            residual = x_cls\n",
        "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
        "            u = self.pre_attn_norm(u)\n",
        "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
        "        else:\n",
        "            residual = x\n",
        "            x = self.pre_attn_norm(x)\n",
        "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask)[0]  # (seq_len, batch, embed_dim)\n",
        "\n",
        "        if self.c_attn is not None:\n",
        "            tgt_len = x.size(0)\n",
        "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
        "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
        "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
        "        if self.post_attn_norm is not None:\n",
        "            x = self.post_attn_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.pre_fc_norm(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act_dropout(x)\n",
        "        if self.post_fc_norm is not None:\n",
        "            x = self.post_fc_norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        if self.w_resid is not None:\n",
        "            residual = torch.mul(self.w_resid, residual)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ParticleTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            if self.for_inference:\n",
        "                output = torch.softmax(output, dim=1)\n",
        "            # print('output:\\n', output)\n",
        "            return output\n",
        "\n",
        "\n",
        "class ParticleTransformerTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask)\n",
        "\n",
        "\n",
        "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if pf_uu_idx is not None:\n",
        "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
        "\n",
        "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
        "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask, uu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BAL5AfiWSKIM"
      },
      "outputs": [],
      "source": [
        "class ParticleTransformerBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AmfBoIrQzR7",
        "outputId": "cde90122-dbb6-4e23-bbff-d0d197f8b083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ParticleTransformerBackbone(\n",
            "  (trimmer): SequenceTrimmer()\n",
            "  (embed): Embed(\n",
            "    (input_bn): BatchNorm1d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (embed): Sequential(\n",
            "      (0): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=19, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (7): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (8): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (pair_embed): PairEmbed(\n",
            "    (embed): Sequential(\n",
            "      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
            "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): GELU(approximate='none')\n",
            "      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): GELU(approximate='none')\n",
            "      (10): Conv1d(64, 2, kernel_size=(1,), stride=(1,))\n",
            "      (11): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (cls_blocks): ModuleList(\n",
            "    (0-1): 2 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# test model\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=20,        # number of jet classes in JetClassII\n",
        "    num_heads=2,           # match default\n",
        "    num_layers=4,          # match default\n",
        ")\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_K5_0EqlSt1r",
        "outputId": "cf4a65c6-9a35-4831-d3da-486441735524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/x5/nk2s0y9j023dsj349jm6v7fh0000gn/T/ipykernel_52199/2604744380.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
            "Epoch 1/50: 4it [00:03,  1.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 5.2572, Accuracy: 0.0100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50: 4it [00:02,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/50, Loss: 4.5776, Accuracy: 0.1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50: 4it [00:01,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/50, Loss: 4.1495, Accuracy: 0.2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50: 4it [00:01,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/50, Loss: 3.8432, Accuracy: 0.2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50: 4it [00:01,  2.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50, Loss: 3.5887, Accuracy: 0.2100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/50: 4it [00:01,  2.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/50, Loss: 3.4057, Accuracy: 0.2400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/50: 4it [00:01,  3.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/50, Loss: 3.2416, Accuracy: 0.2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/50: 4it [00:01,  3.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/50, Loss: 3.0896, Accuracy: 0.2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/50: 4it [00:01,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50, Loss: 2.9447, Accuracy: 0.2700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/50: 4it [00:01,  2.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50, Loss: 2.8057, Accuracy: 0.3100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/50: 4it [00:01,  2.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50, Loss: 2.6584, Accuracy: 0.3200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/50: 4it [00:01,  2.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50, Loss: 2.5346, Accuracy: 0.4300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/50: 4it [00:01,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50, Loss: 2.4351, Accuracy: 0.4800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/50: 4it [00:01,  3.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/50, Loss: 2.3039, Accuracy: 0.5500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/50: 4it [00:01,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50, Loss: 2.1831, Accuracy: 0.5600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/50: 4it [00:01,  2.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50, Loss: 2.0881, Accuracy: 0.5900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/50: 4it [00:01,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50, Loss: 1.9854, Accuracy: 0.6600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/50: 4it [00:01,  3.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/50, Loss: 1.8807, Accuracy: 0.6800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/50: 4it [00:01,  3.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50, Loss: 1.7873, Accuracy: 0.7100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/50: 4it [00:01,  3.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50, Loss: 1.6982, Accuracy: 0.7600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/50: 4it [00:01,  3.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/50, Loss: 1.6064, Accuracy: 0.7800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/50: 4it [00:01,  3.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50, Loss: 1.5205, Accuracy: 0.8400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/50: 4it [00:01,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50, Loss: 1.4230, Accuracy: 0.8700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/50: 4it [00:01,  3.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/50, Loss: 1.3477, Accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/50: 4it [00:01,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50, Loss: 1.2722, Accuracy: 0.9300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/50: 4it [00:01,  3.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/50, Loss: 1.1963, Accuracy: 0.9400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/50: 4it [00:01,  3.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50, Loss: 1.1298, Accuracy: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/50: 4it [00:01,  3.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50, Loss: 1.0637, Accuracy: 0.9900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/50: 4it [00:01,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/50, Loss: 0.9983, Accuracy: 0.9900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/50: 4it [00:01,  3.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50, Loss: 0.9302, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/50: 4it [00:01,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/50, Loss: 0.8728, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/50: 4it [00:01,  3.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/50, Loss: 0.8209, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/50: 4it [00:01,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/50, Loss: 0.7756, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/50: 4it [00:01,  2.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/50, Loss: 0.7439, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/50: 4it [00:01,  3.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/50, Loss: 0.6992, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/50: 4it [00:01,  3.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/50, Loss: 0.6587, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/50: 4it [00:01,  3.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/50, Loss: 0.6221, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/50: 4it [00:01,  3.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/50, Loss: 0.5942, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/50: 4it [00:01,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/50, Loss: 0.5650, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/50: 4it [00:01,  2.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/50, Loss: 0.5361, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/50: 4it [00:01,  3.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/50, Loss: 0.5153, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/50: 4it [00:01,  2.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/50, Loss: 0.4879, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/50: 4it [00:01,  2.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/50, Loss: 0.4829, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/50: 4it [00:01,  2.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/50, Loss: 0.4513, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/50: 4it [00:01,  2.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/50, Loss: 0.4300, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/50: 4it [00:01,  2.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/50, Loss: 0.4106, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/50: 4it [00:01,  2.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/50, Loss: 0.3946, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/50: 4it [00:01,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/50, Loss: 0.3770, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/50: 4it [00:01,  2.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/50, Loss: 0.3672, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/50: 4it [00:01,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/50, Loss: 0.3574, Accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train.py\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-4\n",
        "EPOCHS = 50\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "filepaths = [\"JetClassII_example.parquet\"]\n",
        "train_dataset = IterableJetDataset(filepaths, max_num_particles=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=188,       # number of jet classes in JetClassII\n",
        "    num_layers=1\n",
        "  ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "acc = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  num_batches = 0 \n",
        "  for x_particles, x_jets, v_particles, mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "    x_particles = x_particles.to(device)\n",
        "    v_particles = v_particles.to(device)\n",
        "    mask = mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(\n",
        "            x=x_particles.transpose(1, 2),\n",
        "            v=v_particles.transpose(1, 2),\n",
        "            mask=mask.unsqueeze(1)\n",
        "        )\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    _, pred = outputs.max(1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "    num_batches += 1\n",
        "\n",
        "  epoch_acc = correct / total if total > 0 else 0\n",
        "  epoch_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "  acc.append(epoch_acc)\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), acc, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqUTdB_IsOuT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN57tMsUk+m6rs26eSdvPzt",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
