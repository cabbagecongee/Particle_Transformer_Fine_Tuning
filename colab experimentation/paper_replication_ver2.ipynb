{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNpXofHxLQIj9mKQlPoTLgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabbagecongee/Particle_Transformer_Fine_Tunning/blob/main/paper_replication_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awkward\n",
        "!pip install vector\n",
        "!pip install uproot\n",
        "!pip install weaver-core\n",
        "!pip install fabric"
      ],
      "metadata": {
        "id": "YoUFRK053d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241de538-b2b7-48af-d41d-53b003b5fee3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: awkward in /usr/local/lib/python3.11/dist-packages (2.8.5)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (2025.7.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (8.7.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward) (25.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward) (3.23.0)\n",
            "Requirement already satisfied: vector in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from vector) (1.26.4)\n",
            "Requirement already satisfied: packaging>=19 in /usr/local/lib/python3.11/dist-packages (from vector) (25.0)\n",
            "Requirement already satisfied: uproot in /usr/local/lib/python3.11/dist-packages (5.1.2)\n",
            "Requirement already satisfied: awkward>=2.4.6 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.8.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (25.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (2025.7.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.23.0)\n",
            "Requirement already satisfied: weaver-core in /usr/local/lib/python3.11/dist-packages (0.4.17)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.15.3)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (6.0.2)\n",
            "Requirement already satisfied: awkward0>=0.15.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (0.15.5)\n",
            "Requirement already satisfied: uproot<5.2.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (5.1.2)\n",
            "Requirement already satisfied: awkward>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.8.5)\n",
            "Requirement already satisfied: vector>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.3)\n",
            "Requirement already satisfied: lz4>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.4.4)\n",
            "Requirement already satisfied: xxhash>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.5.0)\n",
            "Requirement already satisfied: tables>=3.6.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.20.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (2025.7.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (8.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (3.6.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (3.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (4.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (6.31.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (75.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.1.3)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.1.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (4.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=1.8.0->weaver-core) (3.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->weaver-core) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->weaver-core) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2025.7.14)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.11/dist-packages (3.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.11/dist-packages (from fabric) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.11/dist-packages (from fabric) (3.5.1)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.11/dist-packages (from fabric) (5.2.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from fabric) (1.2.18)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->fabric) (1.17.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (4.3.0)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/lib/python3/dist-packages (from paramiko>=2.4->fabric) (3.4.8)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.5->paramiko>=2.4->fabric) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.5->paramiko>=2.4->fabric) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile dataloader.py\n",
        "\n",
        "#reference: https://github.com/jet-universe/particle_transformer/blob/main/dataloader.py\n",
        "\n",
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import vector\n",
        "import torch\n",
        "vector.register_awkward()\n",
        "\n",
        "constituent_keys = [\n",
        "    \"part_px\", \"part_py\", \"part_pz\", \"part_energy\",\n",
        "    \"part_deta\", \"part_dphi\",\n",
        "    \"part_d0val\", \"part_d0err\", \"part_dzval\", \"part_dzerr\",\n",
        "    \"part_charge\",\n",
        "    \"part_isElectron\", \"part_isMuon\", \"part_isPhoton\",\n",
        "    \"part_isChargedHadron\", \"part_isNeutralHadron\",\n",
        "    \"part_pt\", \"part_eta\", \"part_phi\"\n",
        "]\n",
        "\n",
        "hlf_keys = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_energy\", \"jet_sdmass\"]\n",
        "label_key = \"jet_label\"\n",
        "\n",
        "def read_file(\n",
        "    filepath,\n",
        "    particle_features,\n",
        "    jet_features,\n",
        "    labels,\n",
        "    max_num_particles=128\n",
        "):\n",
        "  def pad(a, maxlen, value=0, dtype='float32'):\n",
        "    if isinstance(a, np.ndarray) and a.ndim>=2 and a.shape[1] == maxlen:\n",
        "      return a\n",
        "    elif isinstance(a, ak.Array):\n",
        "      if a.ndim ==1:\n",
        "        a = ak.unflatten(a, 1)\n",
        "      a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
        "      return ak.values_astype(a, dtype)\n",
        "    else:\n",
        "      x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
        "      for idx, s in enumerate(a):\n",
        "        if not len(s):\n",
        "          continue\n",
        "        trunc = np.asarray(s[:maxlen], dtype=dtype)\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "      return x\n",
        "\n",
        "  table = ak.Array(ak.from_parquet(filepath))\n",
        "\n",
        "  p4 = vector.zip({\n",
        "      'px': table['part_px'],\n",
        "      'py': table['part_py'],\n",
        "      'pz': table['part_pz'],\n",
        "      'E': table['part_energy']\n",
        "  })\n",
        "\n",
        "  table[\"part_pt\"] = p4.pt\n",
        "  table[\"part_eta\"] = p4.eta\n",
        "  table[\"part_phi\"] = p4.phi\n",
        "\n",
        "\n",
        "  max_num_jets = 2\n",
        "  x_particles = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_jets)) for n in particle_features], axis=1)\n",
        "  x_particles = np.transpose(x_particles, (0, 2, 1))\n",
        "  x_jets = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_jets)) for n in jet_features], axis=1)\n",
        "  x_jets = np.transpose(x_jets, (0, 2, 1))\n",
        "\n",
        "  y = ak.to_numpy(table[label_key]).astype('int64')\n",
        "  # y = np.stack([ak.to_numpy(pad(table[label_key], maxlen=max_num_jets, value=-1, dtype='int64'))], axis=1)\n",
        "\n",
        "  return x_particles, x_jets, y\n",
        "\n",
        "\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, parquet_file, max_num_particles=128):\n",
        "    self.x_particles, self.x_jets, self.labels = read_file(\n",
        "        filepath=parquet_file,\n",
        "        particle_features=constituent_keys,\n",
        "        jet_features=hlf_keys,\n",
        "        labels=label_key\n",
        "        )\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (\n",
        "        torch.tensor(self.x_particles[idx], dtype=torch.float),\n",
        "        torch.tensor(self.x_jets[idx], dtype=torch.float),\n",
        "        torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    )"
      ],
      "metadata": {
        "id": "Rf0q3dv01WUW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-verbose https://github.com/jet-universe/sophon/raw/main/notebooks/JetClassII_example.parquet"
      ],
      "metadata": {
        "id": "ohkYB5_fsURp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a674facb-fcc0-4379-f7bd-4d909858d5ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-22 04:31:41 URL:https://raw.githubusercontent.com/jet-universe/sophon/main/notebooks/JetClassII_example.parquet [447746/447746] -> \"JetClassII_example.parquet\" [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = JetDataset(\"JetClassII_example.parquet\", max_num_particles=128)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "for x_particles, x_jets, labels in dataloader:\n",
        "    print(x_particles.shape) #[batch size, num jets, particle features]\n",
        "    print(x_jets.shape) # [batch size, num jets, jet features (HLFs)]\n",
        "    print(labels.shape) # [batch size]\n",
        "    break\n"
      ],
      "metadata": {
        "id": "TLFre7YdrtsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557e1ef4-6332-4880-b1c2-6203387eb5e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2, 19])\n",
            "torch.Size([32, 2, 5])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "''' Particle Transformer (ParT)\n",
        "\n",
        "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
        "'''\n",
        "\n",
        "#source: https://github.com/hqucms/weaver-core/blob/main/weaver/nn/model/ParticleTransformer.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from weaver.utils.logger import _logger\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_phi(a, b):\n",
        "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_r2(eta1, phi1, eta2, phi2):\n",
        "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
        "\n",
        "\n",
        "def to_pt2(x, eps=1e-8):\n",
        "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        pt2 = pt2.clamp(min=eps)\n",
        "    return pt2\n",
        "\n",
        "\n",
        "def to_m2(x, eps=1e-8):\n",
        "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        m2 = m2.clamp(min=eps)\n",
        "    return m2\n",
        "\n",
        "\n",
        "def atan2(y, x):\n",
        "    sx = torch.sign(x)\n",
        "    sy = torch.sign(y)\n",
        "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
        "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
        "    return atan_part + pi_part\n",
        "\n",
        "\n",
        "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
        "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
        "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
        "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
        "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
        "    if not return_mass:\n",
        "        return torch.cat((pt, rapidity, phi), dim=1)\n",
        "    else:\n",
        "        m = torch.sqrt(to_m2(x, eps=eps))\n",
        "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
        "\n",
        "\n",
        "def boost(x, boostp4, eps=1e-8):\n",
        "    # boost x to the rest frame of boostp4\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
        "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
        "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
        "    gamma2 = (gamma - 1) / b2\n",
        "    gamma2.masked_fill_(b2 == 0, 0)\n",
        "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
        "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
        "    return v\n",
        "\n",
        "\n",
        "def p3_norm(p, eps=1e-8):\n",
        "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
        "\n",
        "\n",
        "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
        "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "\n",
        "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
        "    lndelta = torch.log(delta.clamp(min=eps))\n",
        "    if num_outputs == 1:\n",
        "        return lndelta\n",
        "\n",
        "    if num_outputs > 1:\n",
        "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
        "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
        "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
        "        outputs = [lnkt, lnz, lndelta]\n",
        "\n",
        "    if num_outputs > 3:\n",
        "        xij = xi + xj\n",
        "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
        "        outputs.append(lnm2)\n",
        "\n",
        "    if num_outputs > 4:\n",
        "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
        "        outputs.append(lnds2)\n",
        "\n",
        "    # the following features are not symmetric for (i, j)\n",
        "    if num_outputs > 5:\n",
        "        xj_boost = boost(xj, xij)\n",
        "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
        "        outputs.append(costheta)\n",
        "\n",
        "    if num_outputs > 6:\n",
        "        deltarap = rapi - rapj\n",
        "        deltaphi = delta_phi(phii, phij)\n",
        "        outputs += [deltarap, deltaphi]\n",
        "\n",
        "    assert (len(outputs) == num_outputs)\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "def build_sparse_tensor(uu, idx, seq_len):\n",
        "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
        "    # return: (N, C, seq_len, seq_len)\n",
        "    batch_size, num_fts, num_pairs = uu.size()\n",
        "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
        "    i = torch.cat((\n",
        "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
        "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
        "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "    ), dim=0)\n",
        "    return torch.sparse_coo_tensor(\n",
        "        i, uu.flatten(),\n",
        "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
        "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SequenceTrimmer(nn.Module):\n",
        "\n",
        "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.enabled = enabled\n",
        "        self.target = target\n",
        "        self._counter = 0\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # uu: (N, C', P, P)\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(x[:, :1])\n",
        "        mask = mask.bool()\n",
        "\n",
        "        if self.enabled:\n",
        "            if self._counter < 5:\n",
        "                self._counter += 1\n",
        "            else:\n",
        "                if self.training:\n",
        "                    q = min(1, random.uniform(*self.target))\n",
        "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
        "                    rand = torch.rand_like(mask.type_as(x))\n",
        "                    rand.masked_fill_(~mask, -1)\n",
        "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
        "                    mask = torch.gather(mask, -1, perm)\n",
        "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
        "                    if v is not None:\n",
        "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
        "                    if uu is not None:\n",
        "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
        "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
        "                else:\n",
        "                    maxlen = mask.sum(dim=-1).max()\n",
        "                maxlen = max(maxlen, 1)\n",
        "                if maxlen < mask.size(-1):\n",
        "                    mask = mask[:, :, :maxlen]\n",
        "                    x = x[:, :, :maxlen]\n",
        "                    if v is not None:\n",
        "                        v = v[:, :, :maxlen]\n",
        "                    if uu is not None:\n",
        "                        uu = uu[:, :, :maxlen, :maxlen]\n",
        "\n",
        "        return x, v, mask, uu\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
        "        module_list = []\n",
        "        for dim in dims:\n",
        "            module_list.extend([\n",
        "                nn.LayerNorm(input_dim),\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "            ])\n",
        "            input_dim = dim\n",
        "        self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_bn is not None:\n",
        "            # x: (batch, embed_dim, seq_len)\n",
        "            x = self.input_bn(x)\n",
        "            x = x.permute(2, 0, 1).contiguous()\n",
        "        # x: (seq_len, batch, embed_dim)\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PairEmbed(nn.Module):\n",
        "    def __init__(\n",
        "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
        "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
        "            normalize_input=True, activation='gelu', eps=1e-8,\n",
        "            for_onnx=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pairwise_lv_dim = pairwise_lv_dim\n",
        "        self.pairwise_input_dim = pairwise_input_dim\n",
        "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
        "        self.remove_self_pair = remove_self_pair\n",
        "        self.mode = mode\n",
        "        self.for_onnx = for_onnx\n",
        "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
        "        self.out_dim = dims[-1]\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
        "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "            for dim in dims:\n",
        "                module_list.extend([\n",
        "                    nn.Conv1d(input_dim, dim, 1),\n",
        "                    nn.BatchNorm1d(dim),\n",
        "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                ])\n",
        "                input_dim = dim\n",
        "            if use_pre_activation_pair:\n",
        "                module_list = module_list[:-1]\n",
        "            self.embed = nn.Sequential(*module_list)\n",
        "        elif self.mode == 'sum':\n",
        "            if pairwise_lv_dim > 0:\n",
        "                input_dim = pairwise_lv_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "            if pairwise_input_dim > 0:\n",
        "                input_dim = pairwise_input_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.fts_embed = nn.Sequential(*module_list)\n",
        "        else:\n",
        "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
        "\n",
        "    def forward(self, x, uu=None):\n",
        "        # x: (batch, v_dim, seq_len)\n",
        "        # uu: (batch, v_dim, seq_len, seq_len)\n",
        "        assert (x is not None or uu is not None)\n",
        "        with torch.no_grad():\n",
        "            if x is not None:\n",
        "                batch_size, _, seq_len = x.size()\n",
        "            else:\n",
        "                batch_size, _, seq_len, _ = uu.size()\n",
        "            if self.is_symmetric and not self.for_onnx:\n",
        "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
        "                                          device=(x if x is not None else uu).device)\n",
        "                if x is not None:\n",
        "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
        "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    xj = x[:, :, j, i]\n",
        "                    x = self.pairwise_lv_fts(xi, xj)\n",
        "                if uu is not None:\n",
        "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    uu = uu[:, :, i, j]\n",
        "            else:\n",
        "                if x is not None:\n",
        "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
        "                    if self.remove_self_pair:\n",
        "                        i = torch.arange(0, seq_len, device=x.device)\n",
        "                        x[:, :, i, i] = 0\n",
        "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
        "                if uu is not None:\n",
        "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
        "            if self.mode == 'concat':\n",
        "                if x is None:\n",
        "                    pair_fts = uu\n",
        "                elif uu is None:\n",
        "                    pair_fts = x\n",
        "                else:\n",
        "                    pair_fts = torch.cat((x, uu), dim=1)\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
        "        elif self.mode == 'sum':\n",
        "            if x is None:\n",
        "                elements = self.fts_embed(uu)\n",
        "            elif uu is None:\n",
        "                elements = self.embed(x)\n",
        "            else:\n",
        "                elements = self.embed(x) + self.fts_embed(uu)\n",
        "\n",
        "        if self.is_symmetric and not self.for_onnx:\n",
        "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
        "            y[:, :, i, j] = elements\n",
        "            y[:, :, j, i] = elements\n",
        "        else:\n",
        "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
        "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                 add_bias_kv=False, activation='gelu',\n",
        "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ffn_dim = embed_dim * ffn_ratio\n",
        "\n",
        "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=attn_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "        )\n",
        "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
        "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
        "        self.act_dropout = nn.Dropout(activation_dropout)\n",
        "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
        "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
        "\n",
        "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
        "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
        "\n",
        "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
        "            padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape `(batch, seq_len)` where padding\n",
        "                elements are indicated by ``1``.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "\n",
        "        if x_cls is not None:\n",
        "            with torch.no_grad():\n",
        "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
        "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
        "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
        "            residual = x_cls\n",
        "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
        "            u = self.pre_attn_norm(u)\n",
        "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
        "        else:\n",
        "            residual = x\n",
        "            x = self.pre_attn_norm(x)\n",
        "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask)[0]  # (seq_len, batch, embed_dim)\n",
        "\n",
        "        if self.c_attn is not None:\n",
        "            tgt_len = x.size(0)\n",
        "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
        "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
        "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
        "        if self.post_attn_norm is not None:\n",
        "            x = self.post_attn_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.pre_fc_norm(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act_dropout(x)\n",
        "        if self.post_fc_norm is not None:\n",
        "            x = self.post_fc_norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        if self.w_resid is not None:\n",
        "            residual = torch.mul(self.w_resid, residual)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ParticleTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            if self.for_inference:\n",
        "                output = torch.softmax(output, dim=1)\n",
        "            # print('output:\\n', output)\n",
        "            return output\n",
        "\n",
        "\n",
        "class ParticleTransformerTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask)\n",
        "\n",
        "\n",
        "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if pf_uu_idx is not None:\n",
        "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
        "\n",
        "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
        "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask, uu)"
      ],
      "metadata": {
        "id": "XbfNzEvk1_Hc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParticleTransformerBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 use_hlfs = False,\n",
        "                 hlf_dim=5,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "        self.use_hlfs = use_hlfs\n",
        "        self.hlf_dim = hlf_dim\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        final_dim = embed_dim + (hlf_dim if use_hlfs else 0) #dim of final output with or w/o hlfs\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = final_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None, hlf=None, return_embedding=False):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            if self.use_hlfs:\n",
        "              if hlf is None:\n",
        "                raise ValueError(\"use_hlfs is True but no hlf tensor was provided.\")\n",
        "              if hlf.dim() == 3:\n",
        "                hlf = hlf.reshape(-1, hlf.size(-1))\n",
        "              x_cls = torch.cat([x_cls, hlf], dim=1)\n",
        "\n",
        "            # fc\n",
        "            if return_embedding or self.fc is None:\n",
        "                return x_cls #this is the jet vector\n",
        "\n",
        "            output = self.fc(x_cls) #this is for pretraining with the classification head\n",
        "            return output"
      ],
      "metadata": {
        "id": "BAL5AfiWSKIM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test model\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=188,        # number of jet classes in JetClassII\n",
        "    num_heads=2,           # match default\n",
        "    num_layers=4,          # match default\n",
        ")\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "4AmfBoIrQzR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12ee968-4ab0-46c3-828b-8b70b74d7fb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParticleTransformerBackbone(\n",
            "  (trimmer): SequenceTrimmer()\n",
            "  (embed): Embed(\n",
            "    (input_bn): BatchNorm1d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (embed): Sequential(\n",
            "      (0): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=19, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (7): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (8): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (pair_embed): PairEmbed(\n",
            "    (embed): Sequential(\n",
            "      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
            "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): GELU(approximate='none')\n",
            "      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): GELU(approximate='none')\n",
            "      (10): Conv1d(64, 2, kernel_size=(1,), stride=(1,))\n",
            "      (11): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (cls_blocks): ModuleList(\n",
            "    (0-1): 2 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=188, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddCHJB2WJmaZ",
        "outputId": "2e268252-42f2-4f26-9f4c-44b30221126c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-optimizer\n",
            "  Downloading pytorch_optimizer-3.6.1-py3-none-any.whl.metadata (75 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/75.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-optimizer) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from pytorch-optimizer) (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (2025.7.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->pytorch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->pytorch-optimizer) (3.0.2)\n",
            "Downloading pytorch_optimizer-3.6.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m256.6/256.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-optimizer\n",
            "Successfully installed pytorch-optimizer-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the backbone\n",
        "\n",
        "#the following training is based on parameters specified in https://arxiv.org/pdf/2401.13536\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from optimizer import Lookahead\n",
        "from torch.optim import RAdam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3\n",
        "EPOCHS = 100\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "full_dataset = JetDataset(\"JetClassII_example.parquet\", max_num_particles=128)\n",
        "\n",
        "N = len(full_dataset)\n",
        "train_size = int(0.45 * N)\n",
        "val_size = int(0.05 * N)\n",
        "test_size = N - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "# train_dataset, val_dataset = random_split(full_dataset, [0., 0.1])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=188,       # number of jet classes in JetClassII\n",
        "    use_hlfs = False\n",
        "  ).to(device)\n",
        "\n",
        "\n",
        "def warmup_schedule(step, warmup_steps=1000):\n",
        "    return min(1.0, step / warmup_steps)\n",
        "\n",
        "base_optimizer = RAdam(model.parameters(), lr=1e-3, betas=(0.95, 0.999), eps=1e-5)\n",
        "optimizer = Lookahead(base_optimizer, k=6, alpha=0.5)\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=warmup_schedule)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "acc = []\n",
        "val_acc = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for x_particles, x_jets, labels in tqdm(train_loader):\n",
        "    x_particles, x_jets, labels = x_particles.to(device), x_jets.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(x_particles.transpose(1, 2))\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, pred = outputs.max(1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "  acc.append(correct/total)\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {total_loss/total:.4f}, Train Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "  with torch.inference_mode():\n",
        "      for x_particles, x_jets, labels in val_loader:\n",
        "          x_particles, x_jets, labels = x_particles.to(device), x_jets.to(device), labels.to(device)\n",
        "          outputs = model(x_particles.transpose(1, 2))\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          _, pred = outputs.max(1)\n",
        "          val_correct += (pred == labels).sum().item()\n",
        "          val_total += labels.size(0)\n",
        "\n",
        "  avg_val_loss = val_loss / val_total\n",
        "  val_accuracy = val_correct / val_total\n",
        "  val_acc.append(val_accuracy)\n",
        "\n",
        "  print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "  # save best models\n",
        "  if avg_val_loss < best_val_loss:\n",
        "      best_val_loss = avg_val_loss\n",
        "      torch.save(model.state_dict(), f\"model_at_{epoch}_epochs.pt\")\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, EPOCHS + 1), acc, marker='o', label=\"Train Acc\")\n",
        "plt.plot(range(1, EPOCHS + 1), val_acc, marker='s', label=\"Val Acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training & Validation Accuracy over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.plot(range(1, EPOCHS + 1), acc, marker='o')\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.title(\"Training Accuracy over Epochs\")\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "_K5_0EqlSt1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e7d361-ca9e-42b0-c39e-9894a2e20b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:15<00:00,  3.78s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Train Loss: 0.2185, Train Accuracy: 0.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Train Loss: 0.2141, Train Accuracy: 0.0100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100, Train Loss: 0.2125, Train Accuracy: 0.0100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.18s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100, Train Loss: 0.2053, Train Accuracy: 0.0400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.15s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100, Train Loss: 0.1962, Train Accuracy: 0.0500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:29<00:00,  7.30s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100, Train Loss: 0.1850, Train Accuracy: 0.0900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.40s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100, Train Loss: 0.1847, Train Accuracy: 0.1200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.90s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100, Train Loss: 0.1791, Train Accuracy: 0.1600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.12s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100, Train Loss: 0.1644, Train Accuracy: 0.2400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.75s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Train Loss: 0.1714, Train Accuracy: 0.2700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.10s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100, Train Loss: 0.1572, Train Accuracy: 0.3000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.97s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100, Train Loss: 0.1568, Train Accuracy: 0.3000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.32s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/100, Train Loss: 0.1482, Train Accuracy: 0.3000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.75s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/100, Train Loss: 0.1471, Train Accuracy: 0.3200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.77s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100, Train Loss: 0.1421, Train Accuracy: 0.3500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.10s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100, Train Loss: 0.1476, Train Accuracy: 0.3800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.05s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100, Train Loss: 0.1322, Train Accuracy: 0.4200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.45s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100, Train Loss: 0.1327, Train Accuracy: 0.3700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:23<00:00,  5.90s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100, Train Loss: 0.1288, Train Accuracy: 0.5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.52s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/100, Train Loss: 0.1174, Train Accuracy: 0.4700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.82s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/100, Train Loss: 0.1233, Train Accuracy: 0.5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.22s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/100, Train Loss: 0.1204, Train Accuracy: 0.4700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.03s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/100, Train Loss: 0.1179, Train Accuracy: 0.5600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.83s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100, Train Loss: 0.1075, Train Accuracy: 0.6000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.22s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/100, Train Loss: 0.1150, Train Accuracy: 0.6300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.73s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/100, Train Loss: 0.1016, Train Accuracy: 0.5700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/100, Train Loss: 0.1169, Train Accuracy: 0.6500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.28s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/100, Train Loss: 0.1118, Train Accuracy: 0.6100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.67s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/100, Train Loss: 0.1005, Train Accuracy: 0.6300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100, Train Loss: 0.0950, Train Accuracy: 0.7000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.32s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/100, Train Loss: 0.1036, Train Accuracy: 0.6800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.80s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/100, Train Loss: 0.1009, Train Accuracy: 0.6800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.98s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/100, Train Loss: 0.0952, Train Accuracy: 0.7400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/100, Train Loss: 0.0874, Train Accuracy: 0.7500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.77s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100, Train Loss: 0.0847, Train Accuracy: 0.7000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.93s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/100, Train Loss: 0.0854, Train Accuracy: 0.8000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100, Train Loss: 0.0891, Train Accuracy: 0.7700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.97s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/100, Train Loss: 0.0905, Train Accuracy: 0.8300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.60s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100, Train Loss: 0.0974, Train Accuracy: 0.7700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.00s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/100, Train Loss: 0.0838, Train Accuracy: 0.8100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.70s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/100, Train Loss: 0.0802, Train Accuracy: 0.8600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.80s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/100, Train Loss: 0.0755, Train Accuracy: 0.8100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.15s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/100, Train Loss: 0.0703, Train Accuracy: 0.8900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/100, Train Loss: 0.0675, Train Accuracy: 0.8600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.20s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/100, Train Loss: 0.0751, Train Accuracy: 0.9100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.20s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/100, Train Loss: 0.0706, Train Accuracy: 0.8900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.95s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/100, Train Loss: 0.0658, Train Accuracy: 0.9200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.83s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/100, Train Loss: 0.0662, Train Accuracy: 0.9100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:23<00:00,  5.97s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/100, Train Loss: 0.0563, Train Accuracy: 0.9000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.48s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/100, Train Loss: 0.0585, Train Accuracy: 0.9100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51/100, Train Loss: 0.0501, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52/100, Train Loss: 0.0616, Train Accuracy: 0.9000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53/100, Train Loss: 0.0704, Train Accuracy: 0.8800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.05s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54/100, Train Loss: 0.0628, Train Accuracy: 0.9000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.28s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/100, Train Loss: 0.0678, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.82s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56/100, Train Loss: 0.0679, Train Accuracy: 0.8900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57/100, Train Loss: 0.0595, Train Accuracy: 0.8900\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.30s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/100, Train Loss: 0.0479, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.73s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59/100, Train Loss: 0.0459, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  7.00s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/100, Train Loss: 0.0539, Train Accuracy: 0.9200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.15s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61/100, Train Loss: 0.0381, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62/100, Train Loss: 0.0523, Train Accuracy: 0.9100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63/100, Train Loss: 0.0588, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.20s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64/100, Train Loss: 0.0528, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65/100, Train Loss: 0.0390, Train Accuracy: 0.9200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.17s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66/100, Train Loss: 0.0463, Train Accuracy: 0.9700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:24<00:00,  6.05s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67/100, Train Loss: 0.0464, Train Accuracy: 0.9500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.82s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68/100, Train Loss: 0.0373, Train Accuracy: 0.9700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.95s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69/100, Train Loss: 0.0399, Train Accuracy: 0.9700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.32s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70/100, Train Loss: 0.0394, Train Accuracy: 0.9500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.85s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71/100, Train Loss: 0.0397, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.95s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72/100, Train Loss: 0.0436, Train Accuracy: 0.9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.40s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73/100, Train Loss: 0.0441, Train Accuracy: 0.9500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.90s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74/100, Train Loss: 0.0402, Train Accuracy: 0.9700\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.98s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75/100, Train Loss: 0.0295, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76/100, Train Loss: 0.0541, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  7.00s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77/100, Train Loss: 0.0416, Train Accuracy: 0.9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78/100, Train Loss: 0.0344, Train Accuracy: 0.9500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.27s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/100, Train Loss: 0.0373, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80/100, Train Loss: 0.0432, Train Accuracy: 0.9300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.87s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81/100, Train Loss: 0.0407, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.30s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82/100, Train Loss: 0.0290, Train Accuracy: 0.9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:27<00:00,  6.92s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83/100, Train Loss: 0.0354, Train Accuracy: 0.9800\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:26<00:00,  6.75s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84/100, Train Loss: 0.0492, Train Accuracy: 0.9400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:25<00:00,  6.27s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85/100, Train Loss: 0.0243, Train Accuracy: 0.9600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4/4 [00:28<00:00,  7.07s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86/100, Train Loss: 0.0319, Train Accuracy: 0.8900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 1/4 [00:17<00:53, 17.70s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_particles shape:\", x_particles.shape)\n",
        "print(\"x_jets shape:\", x_jets.shape)\n",
        "print(\"labels shape:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQUFZQ0fbM-W",
        "outputId": "d7745777-4bdc-4b22-d63b-f3aeba1f566c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_particles shape: torch.Size([16, 2, 19])\n",
            "x_jets shape: torch.Size([16, 2, 5])\n",
            "labels shape: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test best model\n",
        "model.load_state_dict(torch.load(\"model_at_XX_epochs.pt\"))\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for x_particles, x_jets, labels in test_loader:\n",
        "        x_particles, x_jets, labels = x_particles.to(device), x_jets.to(device), labels.to(device)\n",
        "        outputs = model(x_particles.transpose(1, 2), hlf=x_jets)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, pred = outputs.max(1)\n",
        "        test_correct += (pred == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}, Test Loss: {avg_test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "S2lP6l2fFKj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}