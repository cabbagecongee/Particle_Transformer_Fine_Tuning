{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabbagecongee/Particle_Transformer_Fine_Tunning/blob/main/model_pipeline_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUFRK053d67",
        "outputId": "1eb70f5c-3245-4e7c-8448-9c2f89564390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: awkward in /usr/local/lib/python3.11/dist-packages (2.8.5)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (8.7.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from awkward) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward) (25.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward) (3.23.0)\n",
            "Requirement already satisfied: vector in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from vector) (1.26.4)\n",
            "Requirement already satisfied: packaging>=19 in /usr/local/lib/python3.11/dist-packages (from vector) (25.0)\n",
            "Requirement already satisfied: uproot in /usr/local/lib/python3.11/dist-packages (5.1.2)\n",
            "Requirement already satisfied: awkward>=2.4.6 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.8.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (25.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.23.0)\n",
            "Requirement already satisfied: weaver-core in /usr/local/lib/python3.11/dist-packages (0.4.17)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.15.3)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (6.0.2)\n",
            "Requirement already satisfied: awkward0>=0.15.5 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (0.15.5)\n",
            "Requirement already satisfied: uproot<5.2.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (5.1.2)\n",
            "Requirement already satisfied: awkward>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.8.5)\n",
            "Requirement already satisfied: vector>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (1.6.3)\n",
            "Requirement already satisfied: lz4>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (4.4.4)\n",
            "Requirement already satisfied: xxhash>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.5.0)\n",
            "Requirement already satisfied: tables>=3.6.1 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (3.10.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from weaver-core) (2.18.0)\n",
            "Requirement already satisfied: awkward-cpp==47 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (47)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (2025.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (8.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward>=1.8.0->weaver-core) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->weaver-core) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.3->weaver-core) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.1->weaver-core) (3.6.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (2.11.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables>=3.6.1->weaver-core) (4.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.2.0->weaver-core) (3.1.3)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.10.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (1.1.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (4.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.32.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=1.8.0->weaver-core) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->weaver-core) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->blosc2>=2.3.0->tables>=3.6.1->weaver-core) (2025.7.14)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.11/dist-packages (3.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.11/dist-packages (from fabric) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.11/dist-packages (from fabric) (3.5.1)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.11/dist-packages (from fabric) (5.2.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from fabric) (1.2.18)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->fabric) (1.17.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (4.3.0)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (43.0.3)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.11/dist-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install awkward\n",
        "!pip install vector\n",
        "!pip install uproot\n",
        "!pip install weaver-core\n",
        "!pip install fabric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rf0q3dv01WUW"
      },
      "outputs": [],
      "source": [
        "# %%writefile dataloader.py\n",
        "\n",
        "#reference: https://github.com/jet-universe/particle_transformer/blob/main/dataloader.py\n",
        "\n",
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import vector\n",
        "import torch\n",
        "vector.register_awkward()\n",
        "\n",
        "constituent_keys = [\n",
        "    \"part_px\", \"part_py\", \"part_pz\", \"part_energy\",\n",
        "    \"part_deta\", \"part_dphi\",\n",
        "    \"part_d0val\", \"part_d0err\", \"part_dzval\", \"part_dzerr\",\n",
        "    \"part_charge\",\n",
        "    \"part_isElectron\", \"part_isMuon\", \"part_isPhoton\",\n",
        "    \"part_isChargedHadron\", \"part_isNeutralHadron\",\n",
        "    \"part_pt\", \"part_eta\", \"part_phi\"\n",
        "]\n",
        "\n",
        "hlf_keys = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_energy\", \"jet_sdmass\"]\n",
        "label_key = \"jet_label\"\n",
        "\n",
        "def read_file(\n",
        "    filepath,\n",
        "    particle_features,\n",
        "    jet_features,\n",
        "    labels,\n",
        "    max_num_particles=128\n",
        "):\n",
        "  def pad(a, maxlen, value=0, dtype='float32'):\n",
        "    if isinstance(a, np.ndarray) and a.ndim>=2 and a.shape[1] == maxlen:\n",
        "      return a\n",
        "    elif isinstance(a, ak.Array):\n",
        "      if a.ndim ==1:\n",
        "        a = ak.unflatten(a, 1)\n",
        "      a = ak.fill_none(ak.pad_none(a, maxlen, clip=True), value)\n",
        "      return ak.values_astype(a, dtype)\n",
        "    else:\n",
        "      x = (np.ones((len(a), maxlen)) * value).astype(dtype)\n",
        "      for idx, s in enumerate(a):\n",
        "        if not len(s):\n",
        "          continue\n",
        "        trunc = np.asarray(s[:maxlen], dtype=dtype)\n",
        "        x[idx, :len(trunc)] = trunc\n",
        "      return x\n",
        "\n",
        "  table = ak.Array(ak.from_parquet(filepath))\n",
        "\n",
        "  p4 = vector.zip({\n",
        "      'px': table['part_px'],\n",
        "      'py': table['part_py'],\n",
        "      'pz': table['part_pz'],\n",
        "      'E': table['part_energy']\n",
        "  })\n",
        "\n",
        "  # ak.with_field(table, \"part_pt\", p4.pt)\n",
        "  # ak.with_field(table, \"part_eta\", p4.eta)\n",
        "  # ak.with_field(table, \"part_phi\", p4.phi)\n",
        "\n",
        "  # table = ak.with_field(table, p4.pt, \"part_pt\")\n",
        "  # table = ak.with_field(table, p4.eta, \"part_eta\")\n",
        "  # table = ak.with_field(table, p4.phi, \"part_phi\")\n",
        "\n",
        "  table[\"part_pt\"] = p4.pt\n",
        "  table[\"part_eta\"] = p4.eta\n",
        "  table[\"part_phi\"] = p4.phi\n",
        "\n",
        "\n",
        "  max_num_jets = 2\n",
        "  x_particles = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_jets)) for n in particle_features], axis=1)\n",
        "  x_particles = np.transpose(x_particles, (0, 2, 1))\n",
        "  # x_jets = np.stack([ak.to_numpy(pad(table[n])) for n in jet_features], axis=1)\n",
        "  x_jets = np.stack([ak.to_numpy(pad(table[n], maxlen=max_num_jets)) for n in jet_features], axis=1)\n",
        "  x_jets = np.transpose(x_jets, (0, 2, 1))\n",
        "\n",
        "  y = ak.to_numpy(table[label_key]).astype('int64')\n",
        "\n",
        "  return x_particles, x_jets, y\n",
        "\n",
        "\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, parquet_file, max_num_particles=128):\n",
        "    self.x_particles, self.x_jets, self.labels = read_file(\n",
        "        filepath=parquet_file,\n",
        "        particle_features=constituent_keys,\n",
        "        jet_features=hlf_keys,\n",
        "        labels=label_key\n",
        "        )\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (\n",
        "        torch.tensor(self.x_particles[idx], dtype=torch.float),\n",
        "        torch.tensor(self.x_jets[idx], dtype=torch.float),\n",
        "        torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohkYB5_fsURp",
        "outputId": "9a4528b7-ef76-4353-a411-8093af80525d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-16 13:09:35 URL:https://raw.githubusercontent.com/jet-universe/sophon/main/notebooks/JetClassII_example.parquet [447746/447746] -> \"JetClassII_example.parquet\" [1]\n"
          ]
        }
      ],
      "source": [
        "! wget --no-verbose https://github.com/jet-universe/sophon/raw/main/notebooks/JetClassII_example.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLFre7YdrtsO",
        "outputId": "ac6bf731-7f1e-46ac-a26b-9ff2b35bf095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 2, 19])\n",
            "torch.Size([32, 2, 5])\n",
            "torch.Size([32])\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "dataset = JetDataset(\"JetClassII_example.parquet\", max_num_particles=128)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "for x_particles, x_jets, labels in dataloader:\n",
        "    print(x_particles.shape) #[batch size, num jets, particle features]\n",
        "    print(x_jets.shape) # [batch size, num jets, jet features (HLFs)]\n",
        "    print(labels.shape) # [batch size]\n",
        "    break\n",
        "\n",
        "print(len(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique classes found: [  0   1   2   3   4   6   8   9  11  14  25  28  29  30  31  33  37  43\n",
            "  44  47  49  51  57  59  60  61  62  70  71  72  73  79  83  86  87  92\n",
            "  93  94  96  99 102 103 104 112 113 114 115 120 128 129 130 133 156 169\n",
            " 179 181 184 185 186 187] \n",
            "Num classes: 60\n"
          ]
        }
      ],
      "source": [
        "unique_classes = np.unique(dataset.labels)\n",
        "print(f\"Unique classes found: {unique_classes} \")\n",
        "print(f\"Num classes: {len(unique_classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XbfNzEvk1_Hc"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "''' Particle Transformer (ParT)\n",
        "\n",
        "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
        "'''\n",
        "\n",
        "#source: https://github.com/hqucms/weaver-core/blob/main/weaver/nn/model/ParticleTransformer.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from weaver.utils.logger import _logger\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_phi(a, b):\n",
        "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_r2(eta1, phi1, eta2, phi2):\n",
        "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
        "\n",
        "\n",
        "def to_pt2(x, eps=1e-8):\n",
        "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        pt2 = pt2.clamp(min=eps)\n",
        "    return pt2\n",
        "\n",
        "\n",
        "def to_m2(x, eps=1e-8):\n",
        "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        m2 = m2.clamp(min=eps)\n",
        "    return m2\n",
        "\n",
        "\n",
        "def atan2(y, x):\n",
        "    sx = torch.sign(x)\n",
        "    sy = torch.sign(y)\n",
        "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
        "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
        "    return atan_part + pi_part\n",
        "\n",
        "\n",
        "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
        "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
        "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
        "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
        "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
        "    if not return_mass:\n",
        "        return torch.cat((pt, rapidity, phi), dim=1)\n",
        "    else:\n",
        "        m = torch.sqrt(to_m2(x, eps=eps))\n",
        "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
        "\n",
        "\n",
        "def boost(x, boostp4, eps=1e-8):\n",
        "    # boost x to the rest frame of boostp4\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
        "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
        "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
        "    gamma2 = (gamma - 1) / b2\n",
        "    gamma2.masked_fill_(b2 == 0, 0)\n",
        "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
        "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
        "    return v\n",
        "\n",
        "\n",
        "def p3_norm(p, eps=1e-8):\n",
        "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
        "\n",
        "\n",
        "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
        "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "\n",
        "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
        "    lndelta = torch.log(delta.clamp(min=eps))\n",
        "    if num_outputs == 1:\n",
        "        return lndelta\n",
        "\n",
        "    if num_outputs > 1:\n",
        "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
        "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
        "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
        "        outputs = [lnkt, lnz, lndelta]\n",
        "\n",
        "    if num_outputs > 3:\n",
        "        xij = xi + xj\n",
        "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
        "        outputs.append(lnm2)\n",
        "\n",
        "    if num_outputs > 4:\n",
        "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
        "        outputs.append(lnds2)\n",
        "\n",
        "    # the following features are not symmetric for (i, j)\n",
        "    if num_outputs > 5:\n",
        "        xj_boost = boost(xj, xij)\n",
        "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
        "        outputs.append(costheta)\n",
        "\n",
        "    if num_outputs > 6:\n",
        "        deltarap = rapi - rapj\n",
        "        deltaphi = delta_phi(phii, phij)\n",
        "        outputs += [deltarap, deltaphi]\n",
        "\n",
        "    assert (len(outputs) == num_outputs)\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "def build_sparse_tensor(uu, idx, seq_len):\n",
        "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
        "    # return: (N, C, seq_len, seq_len)\n",
        "    batch_size, num_fts, num_pairs = uu.size()\n",
        "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
        "    i = torch.cat((\n",
        "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
        "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
        "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "    ), dim=0)\n",
        "    return torch.sparse_coo_tensor(\n",
        "        i, uu.flatten(),\n",
        "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
        "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SequenceTrimmer(nn.Module):\n",
        "\n",
        "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.enabled = enabled\n",
        "        self.target = target\n",
        "        self._counter = 0\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # uu: (N, C', P, P)\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(x[:, :1])\n",
        "        mask = mask.bool()\n",
        "\n",
        "        if self.enabled:\n",
        "            if self._counter < 5:\n",
        "                self._counter += 1\n",
        "            else:\n",
        "                if self.training:\n",
        "                    q = min(1, random.uniform(*self.target))\n",
        "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
        "                    rand = torch.rand_like(mask.type_as(x))\n",
        "                    rand.masked_fill_(~mask, -1)\n",
        "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
        "                    mask = torch.gather(mask, -1, perm)\n",
        "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
        "                    if v is not None:\n",
        "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
        "                    if uu is not None:\n",
        "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
        "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
        "                else:\n",
        "                    maxlen = mask.sum(dim=-1).max()\n",
        "                maxlen = max(maxlen, 1)\n",
        "                if maxlen < mask.size(-1):\n",
        "                    mask = mask[:, :, :maxlen]\n",
        "                    x = x[:, :, :maxlen]\n",
        "                    if v is not None:\n",
        "                        v = v[:, :, :maxlen]\n",
        "                    if uu is not None:\n",
        "                        uu = uu[:, :, :maxlen, :maxlen]\n",
        "\n",
        "        return x, v, mask, uu\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
        "        module_list = []\n",
        "        for dim in dims:\n",
        "            module_list.extend([\n",
        "                nn.LayerNorm(input_dim),\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "            ])\n",
        "            input_dim = dim\n",
        "        self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_bn is not None:\n",
        "            # x: (batch, embed_dim, seq_len)\n",
        "            x = self.input_bn(x)\n",
        "            x = x.permute(2, 0, 1).contiguous()\n",
        "        # x: (seq_len, batch, embed_dim)\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PairEmbed(nn.Module):\n",
        "    def __init__(\n",
        "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
        "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
        "            normalize_input=True, activation='gelu', eps=1e-8,\n",
        "            for_onnx=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pairwise_lv_dim = pairwise_lv_dim\n",
        "        self.pairwise_input_dim = pairwise_input_dim\n",
        "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
        "        self.remove_self_pair = remove_self_pair\n",
        "        self.mode = mode\n",
        "        self.for_onnx = for_onnx\n",
        "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
        "        self.out_dim = dims[-1]\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
        "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "            for dim in dims:\n",
        "                module_list.extend([\n",
        "                    nn.Conv1d(input_dim, dim, 1),\n",
        "                    nn.BatchNorm1d(dim),\n",
        "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                ])\n",
        "                input_dim = dim\n",
        "            if use_pre_activation_pair:\n",
        "                module_list = module_list[:-1]\n",
        "            self.embed = nn.Sequential(*module_list)\n",
        "        elif self.mode == 'sum':\n",
        "            if pairwise_lv_dim > 0:\n",
        "                input_dim = pairwise_lv_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "            if pairwise_input_dim > 0:\n",
        "                input_dim = pairwise_input_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.fts_embed = nn.Sequential(*module_list)\n",
        "        else:\n",
        "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
        "\n",
        "    def forward(self, x, uu=None):\n",
        "        # x: (batch, v_dim, seq_len)\n",
        "        # uu: (batch, v_dim, seq_len, seq_len)\n",
        "        assert (x is not None or uu is not None)\n",
        "        with torch.no_grad():\n",
        "            if x is not None:\n",
        "                batch_size, _, seq_len = x.size()\n",
        "            else:\n",
        "                batch_size, _, seq_len, _ = uu.size()\n",
        "            if self.is_symmetric and not self.for_onnx:\n",
        "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
        "                                          device=(x if x is not None else uu).device)\n",
        "                if x is not None:\n",
        "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
        "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    xj = x[:, :, j, i]\n",
        "                    x = self.pairwise_lv_fts(xi, xj)\n",
        "                if uu is not None:\n",
        "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    uu = uu[:, :, i, j]\n",
        "            else:\n",
        "                if x is not None:\n",
        "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
        "                    if self.remove_self_pair:\n",
        "                        i = torch.arange(0, seq_len, device=x.device)\n",
        "                        x[:, :, i, i] = 0\n",
        "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
        "                if uu is not None:\n",
        "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
        "            if self.mode == 'concat':\n",
        "                if x is None:\n",
        "                    pair_fts = uu\n",
        "                elif uu is None:\n",
        "                    pair_fts = x\n",
        "                else:\n",
        "                    pair_fts = torch.cat((x, uu), dim=1)\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
        "        elif self.mode == 'sum':\n",
        "            if x is None:\n",
        "                elements = self.fts_embed(uu)\n",
        "            elif uu is None:\n",
        "                elements = self.embed(x)\n",
        "            else:\n",
        "                elements = self.embed(x) + self.fts_embed(uu)\n",
        "\n",
        "        if self.is_symmetric and not self.for_onnx:\n",
        "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
        "            y[:, :, i, j] = elements\n",
        "            y[:, :, j, i] = elements\n",
        "        else:\n",
        "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
        "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                 add_bias_kv=False, activation='gelu',\n",
        "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ffn_dim = embed_dim * ffn_ratio\n",
        "\n",
        "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=attn_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "        )\n",
        "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
        "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
        "        self.act_dropout = nn.Dropout(activation_dropout)\n",
        "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
        "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
        "\n",
        "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
        "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
        "\n",
        "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
        "            padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape `(batch, seq_len)` where padding\n",
        "                elements are indicated by ``1``.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "\n",
        "        if x_cls is not None:\n",
        "            with torch.no_grad():\n",
        "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
        "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
        "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
        "            residual = x_cls\n",
        "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
        "            u = self.pre_attn_norm(u)\n",
        "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
        "        else:\n",
        "            residual = x\n",
        "            x = self.pre_attn_norm(x)\n",
        "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask)[0]  # (seq_len, batch, embed_dim)\n",
        "\n",
        "        if self.c_attn is not None:\n",
        "            tgt_len = x.size(0)\n",
        "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
        "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
        "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
        "        if self.post_attn_norm is not None:\n",
        "            x = self.post_attn_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.pre_fc_norm(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act_dropout(x)\n",
        "        if self.post_fc_norm is not None:\n",
        "            x = self.post_fc_norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        if self.w_resid is not None:\n",
        "            residual = torch.mul(self.w_resid, residual)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ParticleTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            if self.for_inference:\n",
        "                output = torch.softmax(output, dim=1)\n",
        "            # print('output:\\n', output)\n",
        "            return output\n",
        "\n",
        "\n",
        "class ParticleTransformerTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask)\n",
        "\n",
        "\n",
        "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if pf_uu_idx is not None:\n",
        "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
        "\n",
        "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
        "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask, uu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BAL5AfiWSKIM"
      },
      "outputs": [],
      "source": [
        "class ParticleTransformerBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            for block in self.blocks:\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AmfBoIrQzR7",
        "outputId": "cde90122-dbb6-4e23-bbff-d0d197f8b083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ParticleTransformerBackbone(\n",
            "  (trimmer): SequenceTrimmer()\n",
            "  (embed): Embed(\n",
            "    (input_bn): BatchNorm1d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (embed): Sequential(\n",
            "      (0): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Linear(in_features=19, out_features=128, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (7): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (8): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (pair_embed): PairEmbed(\n",
            "    (embed): Sequential(\n",
            "      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
            "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): GELU(approximate='none')\n",
            "      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
            "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): GELU(approximate='none')\n",
            "      (10): Conv1d(64, 2, kernel_size=(1,), stride=(1,))\n",
            "      (11): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (cls_blocks): ModuleList(\n",
            "    (0-1): 2 x Block(\n",
            "      (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (act): GELU(approximate='none')\n",
            "      (act_dropout): Dropout(p=0, inplace=False)\n",
            "      (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# test model\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=20,        # number of jet classes in JetClassII\n",
        "    num_heads=2,           # match default\n",
        "    num_layers=4,          # match default\n",
        ")\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_K5_0EqlSt1r",
        "outputId": "cf4a65c6-9a35-4831-d3da-486441735524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipython-input-6-2604744380.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
            "100%|██████████| 4/4 [00:00<00:00, 15.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 5.204445242881775, Accuracy: 0.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Loss: 4.213841915130615, Accuracy: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100, Loss: 3.7867352962493896, Accuracy: 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100, Loss: 3.5388911366462708, Accuracy: 0.22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100, Loss: 3.4783125519752502, Accuracy: 0.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100, Loss: 3.399612069129944, Accuracy: 0.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100, Loss: 2.9394304752349854, Accuracy: 0.28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100, Loss: 2.7710248827934265, Accuracy: 0.26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100, Loss: 2.819369077682495, Accuracy: 0.31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 2.9792444705963135, Accuracy: 0.29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100, Loss: 2.7169524431228638, Accuracy: 0.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100, Loss: 2.5355279445648193, Accuracy: 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/100, Loss: 2.646163761615753, Accuracy: 0.36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/100, Loss: 2.3320305943489075, Accuracy: 0.43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100, Loss: 2.1697849929332733, Accuracy: 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 10.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100, Loss: 2.426579236984253, Accuracy: 0.39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100, Loss: 2.30466228723526, Accuracy: 0.35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100, Loss: 2.435933828353882, Accuracy: 0.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100, Loss: 2.251860797405243, Accuracy: 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/100, Loss: 2.201766550540924, Accuracy: 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/100, Loss: 2.043589234352112, Accuracy: 0.38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/100, Loss: 2.236008107662201, Accuracy: 0.47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/100, Loss: 2.5112985372543335, Accuracy: 0.44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100, Loss: 2.0361118614673615, Accuracy: 0.46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/100, Loss: 2.3032826483249664, Accuracy: 0.38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/100, Loss: 2.3529434204101562, Accuracy: 0.41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/100, Loss: 1.884300947189331, Accuracy: 0.47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/100, Loss: 1.9131424725055695, Accuracy: 0.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/100, Loss: 2.1859328150749207, Accuracy: 0.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100, Loss: 1.766377478837967, Accuracy: 0.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/100, Loss: 1.9413722157478333, Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/100, Loss: 1.6881034076213837, Accuracy: 0.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/100, Loss: 1.92878058552742, Accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/100, Loss: 1.632599651813507, Accuracy: 0.51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100, Loss: 1.8947224915027618, Accuracy: 0.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/100, Loss: 1.6370296478271484, Accuracy: 0.54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100, Loss: 1.5839472115039825, Accuracy: 0.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/100, Loss: 1.667045384645462, Accuracy: 0.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100, Loss: 1.3099512606859207, Accuracy: 0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/100, Loss: 1.293403536081314, Accuracy: 0.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/100, Loss: 1.7941712439060211, Accuracy: 0.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/100, Loss: 1.4593244791030884, Accuracy: 0.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/100, Loss: 1.2929661571979523, Accuracy: 0.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/100, Loss: 1.492218166589737, Accuracy: 0.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/100, Loss: 1.4599473178386688, Accuracy: 0.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/100, Loss: 1.5859447419643402, Accuracy: 0.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/100, Loss: 1.4841665625572205, Accuracy: 0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/100, Loss: 1.5701467394828796, Accuracy: 0.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/100, Loss: 1.3698153495788574, Accuracy: 0.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/100, Loss: 1.835482656955719, Accuracy: 0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51/100, Loss: 1.584011048078537, Accuracy: 0.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52/100, Loss: 1.4041885733604431, Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53/100, Loss: 1.336029201745987, Accuracy: 0.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54/100, Loss: 1.0752241611480713, Accuracy: 0.65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/100, Loss: 1.2817283421754837, Accuracy: 0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56/100, Loss: 1.2964321672916412, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57/100, Loss: 1.4027740061283112, Accuracy: 0.68\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/100, Loss: 1.2854710519313812, Accuracy: 0.65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59/100, Loss: 1.2710549533367157, Accuracy: 0.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/100, Loss: 1.5605494529008865, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61/100, Loss: 1.1940641552209854, Accuracy: 0.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62/100, Loss: 1.0176769942045212, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63/100, Loss: 1.5150475800037384, Accuracy: 0.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 11.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64/100, Loss: 1.2894613593816757, Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65/100, Loss: 1.2087063491344452, Accuracy: 0.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66/100, Loss: 1.0438547283411026, Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67/100, Loss: 1.36907197535038, Accuracy: 0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68/100, Loss: 0.9614179134368896, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69/100, Loss: 1.4665420353412628, Accuracy: 0.73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70/100, Loss: 1.2933120429515839, Accuracy: 0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71/100, Loss: 1.1505855321884155, Accuracy: 0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72/100, Loss: 1.227164849638939, Accuracy: 0.68\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73/100, Loss: 1.005890890955925, Accuracy: 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74/100, Loss: 1.139983892440796, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75/100, Loss: 1.4782191663980484, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76/100, Loss: 0.8862489312887192, Accuracy: 0.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77/100, Loss: 1.0360110849142075, Accuracy: 0.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78/100, Loss: 0.8615489453077316, Accuracy: 0.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/100, Loss: 1.2642322033643723, Accuracy: 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80/100, Loss: 1.2568021416664124, Accuracy: 0.74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81/100, Loss: 1.4213473498821259, Accuracy: 0.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82/100, Loss: 0.9218038320541382, Accuracy: 0.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83/100, Loss: 1.6834489554166794, Accuracy: 0.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84/100, Loss: 1.3221649080514908, Accuracy: 0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85/100, Loss: 1.6553145945072174, Accuracy: 0.67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86/100, Loss: 1.113824501633644, Accuracy: 0.67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87/100, Loss: 0.934223011136055, Accuracy: 0.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88/100, Loss: 0.7929056882858276, Accuracy: 0.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89/100, Loss: 0.7978447526693344, Accuracy: 0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90/100, Loss: 0.9362582415342331, Accuracy: 0.81\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91/100, Loss: 0.8196364641189575, Accuracy: 0.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92/100, Loss: 1.1240517497062683, Accuracy: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93/100, Loss: 0.7481453567743301, Accuracy: 0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94/100, Loss: 0.8501889109611511, Accuracy: 0.85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95/100, Loss: 0.8498865813016891, Accuracy: 0.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96/100, Loss: 0.790395513176918, Accuracy: 0.88\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 15.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97/100, Loss: 1.001051478087902, Accuracy: 0.79\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98/100, Loss: 0.8310821950435638, Accuracy: 0.78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 16.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99/100, Loss: 0.8860564082860947, Accuracy: 0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 17.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/100, Loss: 0.6529735326766968, Accuracy: 0.83\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train.py\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3\n",
        "EPOCHS = 100\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "train_dataset = JetDataset(\"JetClassII_example.parquet\", max_num_particles=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "model = ParticleTransformerBackbone(\n",
        "    input_dim=19,          # number of particle features\n",
        "    num_classes=188       # number of jet classes in JetClassII\n",
        "  ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "acc = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for x_particles, x_jets, labels in tqdm(train_loader):\n",
        "    x_particles, x_jets, labels = x_particles.to(device), x_jets.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(x_particles.transpose(1, 2))\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    _, pred = outputs.max(1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "  acc.append(correct/total)\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader)}, Accuracy: {correct/total}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), acc, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqUTdB_IsOuT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN57tMsUk+m6rs26eSdvPzt",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
